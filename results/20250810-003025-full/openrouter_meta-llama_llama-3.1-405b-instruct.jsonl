{"id": "1", "question": "Stone dust, chess pieces, and a poster that moonlights as a door.", "answer_ref": "The Shawshank Redemption", "aliases": ["Shawshank Redemption"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Shawshank Redemption", "latency_ms": 874, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 79, "output_tokens": 5, "total_tokens": 84}
{"id": "2", "question": "Briefcase with indoor sunrise, diner philosophy, and a dance that solves nothing.", "answer_ref": "Pulp Fiction", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Pulp Fiction", "latency_ms": 871, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 79, "output_tokens": 3, "total_tokens": 82}
{"id": "5", "question": "Two families share an address book but not the same floor.", "answer_ref": "Parasite", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Upstairs Downstairs", "latency_ms": 916, "is_exact": false, "is_alias": false, "fuzzy": 29, "is_correct": false, "input_tokens": 75, "output_tokens": 4, "total_tokens": 79}
{"id": "4", "question": "Parents who sample the buffet pay in porcine installments.", "answer_ref": "Spirited Away", "aliases": ["Sen to Chihiro no Kamikakushi"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Pied Piper of Hamelin", "latency_ms": 960, "is_exact": false, "is_alias": false, "fuzzy": 36, "is_correct": false, "input_tokens": 76, "output_tokens": 6, "total_tokens": 82}
{"id": "3", "question": "Soap-funded therapy where names are withheld and noses aren't.", "answer_ref": "Fight Club", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Sopranos", "latency_ms": 999, "is_exact": false, "is_alias": false, "fuzzy": 0, "is_correct": false, "input_tokens": 75, "output_tokens": 5, "total_tokens": 80}
{"id": "7", "question": "Citywide social experiment involving ferries, coins, and pencils.", "answer_ref": "The Dark Knight", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Seattle", "latency_ms": 768, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false, "input_tokens": 76, "output_tokens": 1, "total_tokens": 77}
{"id": "10", "question": "Blue-collar crew meets uninvited stowaway with dental enthusiasm.", "answer_ref": "Alien", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Alien", "latency_ms": 698, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 77, "output_tokens": 2, "total_tokens": 79}
{"id": "8", "question": "Farming, clocks in closets, and ocean with one extremely punctual wave.", "answer_ref": "Interstellar", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Inception", "latency_ms": 780, "is_exact": false, "is_alias": false, "fuzzy": 38, "is_correct": false, "input_tokens": 79, "output_tokens": 2, "total_tokens": 81}
{"id": "6", "question": "Road trip powered by salt, chrome, and amplified grievances.", "answer_ref": "Mad Max: Fury Road", "aliases": ["Mad Max Fury Road"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Mad Max: Fury Road", "latency_ms": 912, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 5, "total_tokens": 80}
{"id": "9", "question": "Monolith-led fitness program for apes and spacecraft alike.", "answer_ref": "2001: A Space Odyssey", "aliases": ["2001"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "2001: A Space Odyssey", "latency_ms": 846, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 6, "total_tokens": 81}
{"id": "11", "question": "Reformed biker babysits future union leader against liquid competition.", "answer_ref": "Terminator 2: Judgment Day", "aliases": ["T2", "Terminator 2"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117778 tokens, but can only afford 8836. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5231, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false}
{"id": "12", "question": "Teen's history homework graded by photographs and lightning appointments.", "answer_ref": "Back to the Future", "aliases": ["BTTF"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117610 tokens, but can only afford 10098. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5189, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false}
{"id": "13", "question": "Lunch runs late when the chefs turn out to be the menu.", "answer_ref": "Jurassic Park", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117782 tokens, but can only afford 8836. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5191, "is_exact": false, "is_alias": false, "fuzzy": 9, "is_correct": false}
{"id": "14", "question": "Beach town practices seasonal denial until a fin writes the memo.", "answer_ref": "Jaws", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117612 tokens, but can only afford 10098. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5200, "is_exact": false, "is_alias": false, "fuzzy": 2, "is_correct": false}
{"id": "15", "question": "Paperwork, pianos, and airport goodbyes rehearse wartime ethics.", "answer_ref": "Casablanca", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117613 tokens, but can only afford 10098. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5182, "is_exact": false, "is_alias": false, "fuzzy": 6, "is_correct": false}
{"id": "16", "question": "Revenge business with rodents and a suspiciously literate giant.", "answer_ref": "The Princess Bride", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117613 tokens, but can only afford 10061. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5284, "is_exact": false, "is_alias": false, "fuzzy": 11, "is_correct": false}
{"id": "19", "question": "Friendship diluted by shares, depositions, and a hoodie.", "answer_ref": "The Social Network", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117782 tokens, but can only afford 8804. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5185, "is_exact": false, "is_alias": false, "fuzzy": 9, "is_correct": false}
{"id": "17", "question": "Census of moths assists in culinary crime prevention.", "answer_ref": "The Silence of the Lambs", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117846 tokens, but can only afford 11738. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5281, "is_exact": false, "is_alias": false, "fuzzy": 11, "is_correct": false}
{"id": "20", "question": "Metronome used as a weapon in a percussion war.", "answer_ref": "Whiplash", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117848 tokens, but can only afford 11738. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5184, "is_exact": false, "is_alias": false, "fuzzy": 6, "is_correct": false}
{"id": "18", "question": "Detectives follow a cookbook with exactly seven courses.", "answer_ref": "Se7en", "aliases": ["Seven"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "<error: {'message': 'This request requires more credits, or fewer max_tokens. You requested up to 117846 tokens, but can only afford 11738. To increase, visit https://openrouter.ai/settings/credits and add more credits', 'code': 402, 'metadata': {'provider_name': None}}>", "latency_ms": 5322, "is_exact": false, "is_alias": false, "fuzzy": 4, "is_correct": false}
{"id": "21", "question": "Operating system writes love letters when not updating itself.", "answer_ref": "Her", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Her", "latency_ms": 1429, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 1, "total_tokens": 74}
{"id": "22", "question": "Linguistics class with non-linear attendance policy and squid notebooks.", "answer_ref": "Arrival", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Arrival", "latency_ms": 1441, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 2, "total_tokens": 78}
{"id": "23", "question": "Insurance salesman discovers the sky has customer service.", "answer_ref": "The Truman Show", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Truman Show", "latency_ms": 1473, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75}
{"id": "24", "question": "Small town trapped in a calendar bug until empathy patches it.", "answer_ref": "Groundhog Day", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Groundhog Day", "latency_ms": 1487, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 3, "total_tokens": 78}
{"id": "25", "question": "Rug replacement escalates into bowling diplomacy.", "answer_ref": "The Big Lebowski", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Big Lebowski", "latency_ms": 1513, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77}
{"id": "28", "question": "Lobby boy chronicles pastries, prison breaks, and purple uniforms.", "answer_ref": "The Grand Budapest Hotel", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Grand Budapest Hotel", "latency_ms": 553, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 78, "output_tokens": 4, "total_tokens": 82}
{"id": "26", "question": "Coin tosses decide fate; oxygen tank enforces agreements.", "answer_ref": "No Country for Old Men", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Two-Face", "latency_ms": 719, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 75, "output_tokens": 3, "total_tokens": 78}
{"id": "29", "question": "Hidden nation negotiates outreach with feline-themed foreign policy.", "answer_ref": "Black Panther", "aliases": ["Marvel's Black Panther"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Catland", "latency_ms": 681, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77}
{"id": "27", "question": "Milkshakes become a mineral rights seminar.", "answer_ref": "There Will Be Blood", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "There Will Be Blood", "latency_ms": 780, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77}
{"id": "30", "question": "Weekend retreat with tea, chairs, and unwilling houseguests.", "answer_ref": "Get Out", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Shining", "latency_ms": 783, "is_exact": false, "is_alias": false, "fuzzy": 14, "is_correct": false, "input_tokens": 77, "output_tokens": 3, "total_tokens": 80}
{"id": "32", "question": "One city translates the word 'system' into many dialects.", "answer_ref": "The Wire", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Babel", "latency_ms": 735, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false, "input_tokens": 76, "output_tokens": 2, "total_tokens": 78}
{"id": "31", "question": "Meteorite cutlery causes HR issues across several climates.", "answer_ref": "Game of Thrones", "aliases": ["GoT"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Forks", "latency_ms": 854, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 75, "output_tokens": 3, "total_tokens": 78}
{"id": "33", "question": "Therapy sessions where patient confidentiality competes with RICO.", "answer_ref": "The Sopranos", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Sopranos", "latency_ms": 893, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 5, "total_tokens": 81}
{"id": "34", "question": "Suburban bikes explore a basement that forgot its ceiling.", "answer_ref": "Stranger Things", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Goonies", "latency_ms": 854, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false, "input_tokens": 74, "output_tokens": 4, "total_tokens": 78}
{"id": "35", "question": "Paper company proves boredom is a renewable resource.", "answer_ref": "The Office (US)", "aliases": ["The Office US"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Dunder Mifflin", "latency_ms": 852, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 72, "output_tokens": 6, "total_tokens": 78}
{"id": "39", "question": "Modern gadgets audition for the role of ancient curses.", "answer_ref": "Black Mirror", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Ring", "latency_ms": 447, "is_exact": false, "is_alias": false, "fuzzy": 12, "is_correct": false, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "37", "question": "A show about nothing that itemizes everything.", "answer_ref": "Seinfeld", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Seinfeld", "latency_ms": 686, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 2, "total_tokens": 74}
{"id": "36", "question": "Coffeehouse residency with a couch that never graduates.", "answer_ref": "Friends", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Central Perk", "latency_ms": 829, "is_exact": false, "is_alias": false, "fuzzy": 21, "is_correct": false, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76}
{"id": "38", "question": "Owls serve as unreliable narrators for cherry pie mysteries.", "answer_ref": "Twin Peaks", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Twin Peaks", "latency_ms": 681, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 3, "total_tokens": 79}
{"id": "40", "question": "Space western where childcare doubles as payroll.", "answer_ref": "The Mandalorian", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Mandalorian", "latency_ms": 669, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74}
{"id": "41", "question": "Nocturnal philanthropist with cave-based commuting solutions.", "answer_ref": "Batman", "aliases": ["Bruce Wayne"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Batman", "latency_ms": 621, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 1, "total_tokens": 76}
{"id": "42", "question": "Freelance photographer with sticky commute and precognitive tingles.", "answer_ref": "Spider-Man", "aliases": ["Peter Parker", "Spiderman"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Peter Parker", "latency_ms": 660, "is_exact": false, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 2, "total_tokens": 78}
{"id": "45", "question": "Archaeology performed with greater emphasis on cardio than citations.", "answer_ref": "Lara Croft", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Indiana Jones", "latency_ms": 421, "is_exact": false, "is_alias": false, "fuzzy": 17, "is_correct": false, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77}
{"id": "43", "question": "Cigar aficionado with cutlery you can't confiscate.", "answer_ref": "Wolverine", "aliases": ["Logan"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Winston Churchill", "latency_ms": 726, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false, "input_tokens": 77, "output_tokens": 3, "total_tokens": 80}
{"id": "44", "question": "Traveling consultant whose product demos explode on contact.", "answer_ref": "James Bond", "aliases": ["007"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "MythBusters", "latency_ms": 750, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77}
{"id": "46", "question": "Green armored introvert who solves conflicts by respawning less.", "answer_ref": "Master Chief", "aliases": ["John-117"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Hulk", "latency_ms": 656, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77}
{"id": "48", "question": "Freelance pest control for morally ambiguous fairy tales.", "answer_ref": "Geralt of Rivia", "aliases": ["Geralt"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Witcher", "latency_ms": 704, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 74, "output_tokens": 3, "total_tokens": 77}
{"id": "49", "question": "Silent handyman who fixes kingdoms by breaking pots.", "answer_ref": "Link", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Link", "latency_ms": 608, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 1, "total_tokens": 75}
{"id": "47", "question": "Fatherhood meets anger management with mythic collateral.", "answer_ref": "Kratos", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "God of War", "latency_ms": 853, "is_exact": false, "is_alias": false, "fuzzy": 12, "is_correct": false, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76}
{"id": "50", "question": "Engineer whose wearable midlife crisis saves the world.", "answer_ref": "Tony Stark", "aliases": ["Iron Man", "Tony Stark"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Tony Stark", "latency_ms": 688, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 2, "total_tokens": 76}
{"id": "51", "question": "Stick with an HR policy against retirement.", "answer_ref": "Elder Wand", "aliases": ["Deathstick"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Giving Tree", "latency_ms": 694, "is_exact": false, "is_alias": false, "fuzzy": 19, "is_correct": false, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74}
{"id": "52", "question": "Glove that treats population as a configurable setting.", "answer_ref": "Infinity Gauntlet", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Thanos", "latency_ms": 694, "is_exact": false, "is_alias": false, "fuzzy": 17, "is_correct": false, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "53", "question": "Tri-pronged upgrade that lets cars miss deadlines.", "answer_ref": "Flux Capacitor", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Nitrous Oxide", "latency_ms": 792, "is_exact": false, "is_alias": false, "fuzzy": 37, "is_correct": false, "input_tokens": 74, "output_tokens": 4, "total_tokens": 78}
{"id": "54", "question": "Oversized cutlery popular with gravity skeptics in Midgar.", "answer_ref": "Buster Sword", "aliases": ["Cloud's sword"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Buster Sword", "latency_ms": 850, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 78, "output_tokens": 3, "total_tokens": 81}
{"id": "55", "question": "Handheld shortcut that turns floors into suggestions.", "answer_ref": "Portal Gun", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Ouija Board", "latency_ms": 761, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76}
{"id": "56", "question": "Weighted friendship that fails basic combustion tests.", "answer_ref": "Companion Cube", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Flintstones", "latency_ms": 695, "is_exact": false, "is_alias": false, "fuzzy": 32, "is_correct": false, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75}
{"id": "57", "question": "Municipal planning by pharmaceutical committee goes poorly.", "answer_ref": "Raccoon City", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Jonestown", "latency_ms": 850, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76}
{"id": "59", "question": "Anytown USA with a five-finger discount on anatomy.", "answer_ref": "Springfield", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Body Worlds", "latency_ms": 678, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77}
{"id": "58", "question": "Urban nightlight attracts themed criminals like moths.", "answer_ref": "Gotham City", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Gotham City", "latency_ms": 790, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77}
{"id": "60", "question": "Mountain resort with a generous winter rate for ghosts.", "answer_ref": "The Overlook Hotel", "aliases": ["Overlook Hotel"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Shining", "latency_ms": 707, "is_exact": false, "is_alias": false, "fuzzy": 9, "is_correct": false, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76}
{"id": "61", "question": "Dictionary on a diet and screens that watch back.", "answer_ref": "1984", "aliases": ["Nineteen Eighty-Four"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Kindle", "latency_ms": 874, "is_exact": false, "is_alias": false, "fuzzy": 23, "is_correct": false, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "62", "question": "Dry world where rhythms of sand set politics and prophecy.", "answer_ref": "Dune", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Dune", "latency_ms": 597, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 2, "total_tokens": 76}
{"id": "63", "question": "Unexpected travel invoice itemized by songs, maps, and barrels.", "answer_ref": "The Hobbit", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Hobbit", "latency_ms": 730, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 3, "total_tokens": 79}
{"id": "65", "question": "Musician studies physics to pronounce reality correctly.", "answer_ref": "The Name of the Wind", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Stephen Hawking", "latency_ms": 731, "is_exact": false, "is_alias": false, "fuzzy": 29, "is_correct": false, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75}
{"id": "64", "question": "Teen critiques phonies while auditioning for solitude.", "answer_ref": "The Catcher in the Rye", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Catcher in the Rye", "latency_ms": 961, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 7, "total_tokens": 80}
{"id": "66", "question": "Opera, rock, and confession crammed into one late-night phone call.", "answer_ref": "Bohemian Rhapsody", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Primal Scream", "latency_ms": 862, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 78, "output_tokens": 4, "total_tokens": 82}
{"id": "67", "question": "Red jacket, moonwalk, and a zombie rehearsal dinner.", "answer_ref": "Thriller", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Thriller", "latency_ms": 792, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77}
{"id": "68", "question": "Promise-heavy synth pledge weaponized by surprise.", "answer_ref": "Never Gonna Give You Up", "aliases": ["Rickroll song"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Blade Runner", "latency_ms": 693, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75}
{"id": "70", "question": "Horse-free riding lesson conducted in a tux.", "answer_ref": "Gangnam Style", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Fred Astaire", "latency_ms": 460, "is_exact": false, "is_alias": false, "fuzzy": 32, "is_correct": false, "input_tokens": 74, "output_tokens": 3, "total_tokens": 77}
{"id": "69", "question": "Water metaphors filed under overdue heartbreak invoices.", "answer_ref": "Rolling in the Deep", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Eternal Sunshine of the Spotless Mind", "latency_ms": 867, "is_exact": false, "is_alias": false, "fuzzy": 41, "is_correct": false, "input_tokens": 73, "output_tokens": 8, "total_tokens": 81}
{"id": "71", "question": "Falling real estate teaches ruthless minimalism.", "answer_ref": "Tetris", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Fight Club", "latency_ms": 716, "is_exact": false, "is_alias": false, "fuzzy": 12, "is_correct": false, "input_tokens": 72, "output_tokens": 2, "total_tokens": 74}
{"id": "72", "question": "Cubist wilderness where punching trees becomes architecture.", "answer_ref": "Minecraft", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Minecraft", "latency_ms": 719, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "74", "question": "Open-world archaeology of despair, punctuated by grace checkpoints.", "answer_ref": "Elden Ring", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Dark Souls", "latency_ms": 683, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 76, "output_tokens": 2, "total_tokens": 78}
{"id": "73", "question": "Space crew plays HR's least favorite trust exercise.", "answer_ref": "Among Us", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Alien", "latency_ms": 688, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "75", "question": "Death-as-syllabus with bonfires for office hours.", "answer_ref": "Dark Souls", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Thanos", "latency_ms": 650, "is_exact": false, "is_alias": false, "fuzzy": 37, "is_correct": false, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77}
{"id": "80", "question": "Interstellar HR balancing romance, diplomacy, and space bugs.", "answer_ref": "Mass Effect", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Starship Troopers", "latency_ms": 527, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 75, "output_tokens": 4, "total_tokens": 79}
{"id": "76", "question": "Adoption paperwork scattered across war and alchemy.", "answer_ref": "The Witcher 3: Wild Hunt", "aliases": ["The Witcher 3"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Fullmetal Alchemist", "latency_ms": 795, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77}
{"id": "79", "question": "Science fair judged by cake lore and lemons.", "answer_ref": "Portal 2", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Nobel Prize", "latency_ms": 746, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76}
{"id": "77", "question": "Campfire ledger balances loyalty, debt, and westward entropy.", "answer_ref": "Red Dead Redemption 2", "aliases": ["RDR2"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Ox-Bow Incident", "latency_ms": 829, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false, "input_tokens": 76, "output_tokens": 5, "total_tokens": 81}
{"id": "78", "question": "Three unreliable narrators share a commute and a crime spreadsheet.", "answer_ref": "Grand Theft Auto V", "aliases": ["GTA V"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Girl on the Train", "latency_ms": 823, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 75, "output_tokens": 5, "total_tokens": 80}
{"id": "83", "question": "Teenage strategist whose mail came with flames.", "answer_ref": "Joan of Arc", "aliases": ["Jeanne d'Arc"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Ender Wiggin", "latency_ms": 756, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76}
{"id": "85", "question": "Hair that suggests stormy weather for equations.", "answer_ref": "Albert Einstein", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Albert Einstein", "latency_ms": 682, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 2, "total_tokens": 74}
{"id": "82", "question": "Postal service enthusiast with aggressive merger policies.", "answer_ref": "Genghis Khan", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Vladimir Putin", "latency_ms": 818, "is_exact": false, "is_alias": false, "fuzzy": 23, "is_correct": false, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75}
{"id": "84", "question": "Glowing research with a half-life of acclaim.", "answer_ref": "Marie Curie", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Marie Curie", "latency_ms": 827, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 4, "total_tokens": 78}
{"id": "81", "question": "Short on continents, tall on ambitions and tailored coats.", "answer_ref": "Napoleon Bonaparte", "aliases": ["Napoleon"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Napoleon Bonaparte", "latency_ms": 913, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 6, "total_tokens": 80}
{"id": "86", "question": "One-eyed project manager who pays ravens in secrets.", "answer_ref": "Odin", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Odin", "latency_ms": 791, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 2, "total_tokens": 76}
{"id": "87", "question": "Strategy consultant whose owl handles night shift.", "answer_ref": "Athena", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Athene", "latency_ms": 794, "is_exact": false, "is_alias": false, "fuzzy": 83, "is_correct": false, "input_tokens": 71, "output_tokens": 2, "total_tokens": 73}
{"id": "90", "question": "Clocks take a nap over a hot afternoon.", "answer_ref": "The Persistence of Memory", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "High Noon", "latency_ms": 714, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "88", "question": "Underworld landlord with a pomegranate clause.", "answer_ref": "Hades", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Hades", "latency_ms": 810, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 2, "total_tokens": 76}
{"id": "89", "question": "Village rooflines under a swirling insomnia.", "answer_ref": "Starry Night", "aliases": ["The Starry Night"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Starry Night", "latency_ms": 797, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75}
{"id": "92", "question": "Shiba evaluates life with quantifiers and Comic Sans.", "answer_ref": "Doge", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Doge", "latency_ms": 699, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 2, "total_tokens": 76}
{"id": "95", "question": "Finish line receives delivery before the starting gun's echo.", "answer_ref": "Usain Bolt", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "FedEx", "latency_ms": 665, "is_exact": false, "is_alias": false, "fuzzy": 0, "is_correct": false, "input_tokens": 74, "output_tokens": 2, "total_tokens": 76}
{"id": "93", "question": "Airborne accountant who tallied rings in Chicago.", "answer_ref": "Michael Jordan", "aliases": ["MJ"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Michael Jordan", "latency_ms": 704, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "91", "question": "Allegiance wanders mid-stride; captions do the rest.", "answer_ref": "Distracted Boyfriend", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Benedict Arnold", "latency_ms": 750, "is_exact": false, "is_alias": false, "fuzzy": 45, "is_correct": false, "input_tokens": 77, "output_tokens": 4, "total_tokens": 81}
{"id": "94", "question": "Serve so decisive it files its own appeal.", "answer_ref": "Serena Williams", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Serena Williams", "latency_ms": 732, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75}
{"id": "97", "question": "Patriotic discus enthusiast unfrozen for modern meetings.", "answer_ref": "Captain America", "aliases": ["Steve Rogers"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Captain America", "latency_ms": 471, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77}
{"id": "96", "question": "Purple ethicist proposes resource management by finger interface.", "answer_ref": "Thanos", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Barney", "latency_ms": 659, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "98", "question": "Stand-up philosophy major specializing in citywide punchlines.", "answer_ref": "Joker", "aliases": ["The Joker"], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Seinfeld", "latency_ms": 697, "is_exact": false, "is_alias": false, "fuzzy": 15, "is_correct": false, "input_tokens": 74, "output_tokens": 2, "total_tokens": 76}
{"id": "99", "question": "Imprisonment catered by dumplings; vengeance plated cold.", "answer_ref": "Oldboy", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "Oldboy", "latency_ms": 698, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 77, "output_tokens": 2, "total_tokens": 79}
{"id": "100", "question": "Typewriter productivity meets corridor geometry and mirrored warnings.", "answer_ref": "The Shining", "aliases": [], "model": "openrouter:meta-llama/llama-3.1-405b-instruct", "answer": "The Shining", "latency_ms": 705, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76}
