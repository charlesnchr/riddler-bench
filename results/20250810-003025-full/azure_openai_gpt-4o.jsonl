{"id": "8", "question": "Farming, clocks in closets, and ocean with one extremely punctual wave.", "answer_ref": "Interstellar", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Interstellar", "latency_ms": 2124, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 3, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "4", "question": "Parents who sample the buffet pay in porcine installments.", "answer_ref": "Spirited Away", "aliases": ["Sen to Chihiro no Kamikakushi"], "model": "azure_openai:gpt-4o", "answer": "Piggy Bank", "latency_ms": 2162, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "6", "question": "Road trip powered by salt, chrome, and amplified grievances.", "answer_ref": "Mad Max: Fury Road", "aliases": ["Mad Max Fury Road"], "model": "azure_openai:gpt-4o", "answer": "Mad Max", "latency_ms": 2162, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "5", "question": "Two families share an address book but not the same floor.", "answer_ref": "Parasite", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Duplex", "latency_ms": 2180, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "2", "question": "Briefcase with indoor sunrise, diner philosophy, and a dance that solves nothing.", "answer_ref": "Pulp Fiction", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Pulp Fiction", "latency_ms": 2283, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 4, "total_tokens": 80, "reasoning_tokens": 0}
{"id": "3", "question": "Soap-funded therapy where names are withheld and noses aren't.", "answer_ref": "Fight Club", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Fight Club", "latency_ms": 2638, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "7", "question": "Citywide social experiment involving ferries, coins, and pencils.", "answer_ref": "The Dark Knight", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Dark Knight", "latency_ms": 2727, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "13", "question": "Lunch runs late when the chefs turn out to be the menu.", "answer_ref": "Jurassic Park", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "<error: Azure has not provided the response due to a content filter being triggered>", "latency_ms": 486, "is_exact": false, "is_alias": false, "fuzzy": 18, "is_correct": false}
{"id": "1", "question": "Stone dust, chess pieces, and a poster that moonlights as a door.", "answer_ref": "The Shawshank Redemption", "aliases": ["Shawshank Redemption"], "model": "azure_openai:gpt-4o", "answer": "The Chronicles of Narnia", "latency_ms": 2842, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 76, "output_tokens": 6, "total_tokens": 82, "reasoning_tokens": 0}
{"id": "10", "question": "Blue-collar crew meets uninvited stowaway with dental enthusiasm.", "answer_ref": "Alien", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Alien", "latency_ms": 954, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "12", "question": "Teen's history homework graded by photographs and lightning appointments.", "answer_ref": "Back to the Future", "aliases": ["BTTF"], "model": "azure_openai:gpt-4o", "answer": "Benjamin Franklin", "latency_ms": 942, "is_exact": false, "is_alias": false, "fuzzy": 32, "is_correct": false, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "14", "question": "Beach town practices seasonal denial until a fin writes the memo.", "answer_ref": "Jaws", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Amity Island", "latency_ms": 495, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "11", "question": "Reformed biker babysits future union leader against liquid competition.", "answer_ref": "Terminator 2: Judgment Day", "aliases": ["T2", "Terminator 2"], "model": "azure_openai:gpt-4o", "answer": "Terminator 2", "latency_ms": 984, "is_exact": false, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "9", "question": "Monolith-led fitness program for apes and spacecraft alike.", "answer_ref": "2001: A Space Odyssey", "aliases": ["2001"], "model": "azure_openai:gpt-4o", "answer": "2001: A Space Odyssey", "latency_ms": 1026, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 7, "total_tokens": 79, "reasoning_tokens": 0}
{"id": "16", "question": "Revenge business with rodents and a suspiciously literate giant.", "answer_ref": "The Princess Bride", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Princess Bride", "latency_ms": 498, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 4, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "17", "question": "Census of moths assists in culinary crime prevention.", "answer_ref": "The Silence of the Lambs", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Butterfly Effect", "latency_ms": 618, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "22", "question": "Linguistics class with non-linear attendance policy and squid notebooks.", "answer_ref": "Arrival", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Arrival", "latency_ms": 480, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "21", "question": "Operating system writes love letters when not updating itself.", "answer_ref": "Her", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Windows", "latency_ms": 489, "is_exact": false, "is_alias": false, "fuzzy": 0, "is_correct": false, "input_tokens": 70, "output_tokens": 2, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "18", "question": "Detectives follow a cookbook with exactly seven courses.", "answer_ref": "Se7en", "aliases": ["Seven"], "model": "azure_openai:gpt-4o", "answer": "Seven.", "latency_ms": 525, "is_exact": false, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "20", "question": "Metronome used as a weapon in a percussion war.", "answer_ref": "Whiplash", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Whiplash", "latency_ms": 529, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "19", "question": "Friendship diluted by shares, depositions, and a hoodie.", "answer_ref": "The Social Network", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Social Network", "latency_ms": 555, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "15", "question": "Paperwork, pianos, and airport goodbyes rehearse wartime ethics.", "answer_ref": "Casablanca", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Casablanca", "latency_ms": 1022, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 77, "output_tokens": 3, "total_tokens": 80, "reasoning_tokens": 0}
{"id": "23", "question": "Insurance salesman discovers the sky has customer service.", "answer_ref": "The Truman Show", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Truman Show", "latency_ms": 516, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "24", "question": "Small town trapped in a calendar bug until empathy patches it.", "answer_ref": "Groundhog Day", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Groundhog Day", "latency_ms": 504, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "27", "question": "Milkshakes become a mineral rights seminar.", "answer_ref": "There Will Be Blood", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "There Will Be Blood", "latency_ms": 517, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 5, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "29", "question": "Hidden nation negotiates outreach with feline-themed foreign policy.", "answer_ref": "Black Panther", "aliases": ["Marvel's Black Panther"], "model": "azure_openai:gpt-4o", "answer": "Catland", "latency_ms": 492, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "28", "question": "Lobby boy chronicles pastries, prison breaks, and purple uniforms.", "answer_ref": "The Grand Budapest Hotel", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Grand Budapest Hotel", "latency_ms": 522, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "26", "question": "Coin tosses decide fate; oxygen tank enforces agreements.", "answer_ref": "No Country for Old Men", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "No Country for Old Men", "latency_ms": 548, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 6, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "30", "question": "Weekend retreat with tea, chairs, and unwilling houseguests.", "answer_ref": "Get Out", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Get Out", "latency_ms": 499, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "31", "question": "Meteorite cutlery causes HR issues across several climates.", "answer_ref": "Game of Thrones", "aliases": ["GoT"], "model": "azure_openai:gpt-4o", "answer": "Cloudy with a Chance of Meatballs", "latency_ms": 561, "is_exact": false, "is_alias": false, "fuzzy": 39, "is_correct": false, "input_tokens": 71, "output_tokens": 9, "total_tokens": 80, "reasoning_tokens": 0}
{"id": "32", "question": "One city translates the word 'system' into many dialects.", "answer_ref": "The Wire", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "New York", "latency_ms": 487, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "25", "question": "Rug replacement escalates into bowling diplomacy.", "answer_ref": "The Big Lebowski", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Big Lebowski", "latency_ms": 1007, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 5, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "36", "question": "Coffeehouse residency with a couch that never graduates.", "answer_ref": "Friends", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Friends", "latency_ms": 474, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 2, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "34", "question": "Suburban bikes explore a basement that forgot its ceiling.", "answer_ref": "Stranger Things", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Stranger Things", "latency_ms": 498, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "33", "question": "Therapy sessions where patient confidentiality competes with RICO.", "answer_ref": "The Sopranos", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Sopranos", "latency_ms": 524, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "37", "question": "A show about nothing that itemizes everything.", "answer_ref": "Seinfeld", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Seinfeld", "latency_ms": 447, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "38", "question": "Owls serve as unreliable narrators for cherry pie mysteries.", "answer_ref": "Twin Peaks", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Twin Peaks", "latency_ms": 489, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "39", "question": "Modern gadgets audition for the role of ancient curses.", "answer_ref": "Black Mirror", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Black Mirror", "latency_ms": 500, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "41", "question": "Nocturnal philanthropist with cave-based commuting solutions.", "answer_ref": "Batman", "aliases": ["Bruce Wayne"], "model": "azure_openai:gpt-4o", "answer": "Batman", "latency_ms": 466, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 2, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "35", "question": "Paper company proves boredom is a renewable resource.", "answer_ref": "The Office (US)", "aliases": ["The Office US"], "model": "azure_openai:gpt-4o", "answer": "Dunder Mifflin", "latency_ms": 956, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 69, "output_tokens": 7, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "40", "question": "Space western where childcare doubles as payroll.", "answer_ref": "The Mandalorian", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Mandalorian", "latency_ms": 523, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 68, "output_tokens": 5, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "42", "question": "Freelance photographer with sticky commute and precognitive tingles.", "answer_ref": "Spider-Man", "aliases": ["Peter Parker", "Spiderman"], "model": "azure_openai:gpt-4o", "answer": "Spider-Man", "latency_ms": 512, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "43", "question": "Cigar aficionado with cutlery you can't confiscate.", "answer_ref": "Wolverine", "aliases": ["Logan"], "model": "azure_openai:gpt-4o", "answer": "Wolverine", "latency_ms": 520, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "45", "question": "Archaeology performed with greater emphasis on cardio than citations.", "answer_ref": "Lara Croft", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Indiana Jones", "latency_ms": 499, "is_exact": false, "is_alias": false, "fuzzy": 17, "is_correct": false, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "46", "question": "Green armored introvert who solves conflicts by respawning less.", "answer_ref": "Master Chief", "aliases": ["John-117"], "model": "azure_openai:gpt-4o", "answer": "Master Chief", "latency_ms": 496, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "44", "question": "Traveling consultant whose product demos explode on contact.", "answer_ref": "James Bond", "aliases": ["007"], "model": "azure_openai:gpt-4o", "answer": "Ethan Hunt", "latency_ms": 852, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 70, "output_tokens": 4, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "48", "question": "Freelance pest control for morally ambiguous fairy tales.", "answer_ref": "Geralt of Rivia", "aliases": ["Geralt"], "model": "azure_openai:gpt-4o", "answer": "Shrek", "latency_ms": 472, "is_exact": false, "is_alias": false, "fuzzy": 18, "is_correct": false, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "47", "question": "Fatherhood meets anger management with mythic collateral.", "answer_ref": "Kratos", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "God of War", "latency_ms": 503, "is_exact": false, "is_alias": false, "fuzzy": 12, "is_correct": false, "input_tokens": 70, "output_tokens": 4, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "49", "question": "Silent handyman who fixes kingdoms by breaking pots.", "answer_ref": "Link", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Link", "latency_ms": 477, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 2, "total_tokens": 71, "reasoning_tokens": 0}
{"id": "50", "question": "Engineer whose wearable midlife crisis saves the world.", "answer_ref": "Tony Stark", "aliases": ["Iron Man", "Tony Stark"], "model": "azure_openai:gpt-4o", "answer": "Iron Man", "latency_ms": 497, "is_exact": false, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "51", "question": "Stick with an HR policy against retirement.", "answer_ref": "Elder Wand", "aliases": ["Deathstick"], "model": "azure_openai:gpt-4o", "answer": "Logan's Run", "latency_ms": 516, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 68, "output_tokens": 5, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "52", "question": "Glove that treats population as a configurable setting.", "answer_ref": "Infinity Gauntlet", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Infinity Gauntlet", "latency_ms": 497, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 5, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "53", "question": "Tri-pronged upgrade that lets cars miss deadlines.", "answer_ref": "Flux Capacitor", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Traffic light", "latency_ms": 481, "is_exact": false, "is_alias": false, "fuzzy": 29, "is_correct": false, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "56", "question": "Weighted friendship that fails basic combustion tests.", "answer_ref": "Companion Cube", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Heavy Water", "latency_ms": 495, "is_exact": false, "is_alias": false, "fuzzy": 24, "is_correct": false, "input_tokens": 68, "output_tokens": 3, "total_tokens": 71, "reasoning_tokens": 0}
{"id": "57", "question": "Municipal planning by pharmaceutical committee goes poorly.", "answer_ref": "Raccoon City", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Zootopia", "latency_ms": 489, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "58", "question": "Urban nightlight attracts themed criminals like moths.", "answer_ref": "Gotham City", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Gotham City", "latency_ms": 520, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 5, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "60", "question": "Mountain resort with a generous winter rate for ghosts.", "answer_ref": "The Overlook Hotel", "aliases": ["Overlook Hotel"], "model": "azure_openai:gpt-4o", "answer": "The Overlook Hotel", "latency_ms": 503, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 5, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "54", "question": "Oversized cutlery popular with gravity skeptics in Midgar.", "answer_ref": "Buster Sword", "aliases": ["Cloud's sword"], "model": "azure_openai:gpt-4o", "answer": "Buster Sword", "latency_ms": 789, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "61", "question": "Dictionary on a diet and screens that watch back.", "answer_ref": "1984", "aliases": ["Nineteen Eighty-Four"], "model": "azure_openai:gpt-4o", "answer": "Kindle", "latency_ms": 493, "is_exact": false, "is_alias": false, "fuzzy": 23, "is_correct": false, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "55", "question": "Handheld shortcut that turns floors into suggestions.", "answer_ref": "Portal Gun", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Vacuum cleaner", "latency_ms": 954, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "62", "question": "Dry world where rhythms of sand set politics and prophecy.", "answer_ref": "Dune", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Dune", "latency_ms": 484, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "59", "question": "Anytown USA with a five-finger discount on anatomy.", "answer_ref": "Springfield", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Body Snatchers", "latency_ms": 934, "is_exact": false, "is_alias": false, "fuzzy": 24, "is_correct": false, "input_tokens": 73, "output_tokens": 5, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "63", "question": "Unexpected travel invoice itemized by songs, maps, and barrels.", "answer_ref": "The Hobbit", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Around the World in 80 Days", "latency_ms": 563, "is_exact": false, "is_alias": false, "fuzzy": 13, "is_correct": false, "input_tokens": 73, "output_tokens": 8, "total_tokens": 81, "reasoning_tokens": 0}
{"id": "66", "question": "Opera, rock, and confession crammed into one late-night phone call.", "answer_ref": "Bohemian Rhapsody", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Bohemian Rhapsody", "latency_ms": 503, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 6, "total_tokens": 81, "reasoning_tokens": 0}
{"id": "67", "question": "Red jacket, moonwalk, and a zombie rehearsal dinner.", "answer_ref": "Thriller", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Thriller", "latency_ms": 487, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "69", "question": "Water metaphors filed under overdue heartbreak invoices.", "answer_ref": "Rolling in the Deep", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Notebook", "latency_ms": 466, "is_exact": false, "is_alias": false, "fuzzy": 17, "is_correct": false, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "68", "question": "Promise-heavy synth pledge weaponized by surprise.", "answer_ref": "Never Gonna Give You Up", "aliases": ["Rickroll song"], "model": "azure_openai:gpt-4o", "answer": "Trojan Horse", "latency_ms": 522, "is_exact": false, "is_alias": false, "fuzzy": 32, "is_correct": false, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "70", "question": "Horse-free riding lesson conducted in a tux.", "answer_ref": "Gangnam Style", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Black Tie", "latency_ms": 492, "is_exact": false, "is_alias": false, "fuzzy": 36, "is_correct": false, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "64", "question": "Teen critiques phonies while auditioning for solitude.", "answer_ref": "The Catcher in the Rye", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "<error: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'high'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}>", "latency_ms": 965, "is_exact": false, "is_alias": false, "fuzzy": 6, "is_correct": false}
{"id": "65", "question": "Musician studies physics to pronounce reality correctly.", "answer_ref": "The Name of the Wind", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Brian May", "latency_ms": 833, "is_exact": false, "is_alias": false, "fuzzy": 19, "is_correct": false, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "71", "question": "Falling real estate teaches ruthless minimalism.", "answer_ref": "Tetris", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Fight Club", "latency_ms": 510, "is_exact": false, "is_alias": false, "fuzzy": 12, "is_correct": false, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "72", "question": "Cubist wilderness where punching trees becomes architecture.", "answer_ref": "Minecraft", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Minecraft", "latency_ms": 461, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 2, "total_tokens": 71, "reasoning_tokens": 0}
{"id": "73", "question": "Space crew plays HR's least favorite trust exercise.", "answer_ref": "Among Us", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Among Us", "latency_ms": 479, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "75", "question": "Death-as-syllabus with bonfires for office hours.", "answer_ref": "Dark Souls", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Dark Souls", "latency_ms": 484, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "74", "question": "Open-world archaeology of despair, punctuated by grace checkpoints.", "answer_ref": "Elden Ring", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Elden Ring", "latency_ms": 515, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "79", "question": "Science fair judged by cake lore and lemons.", "answer_ref": "Portal 2", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Great British Bake Off", "latency_ms": 483, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 69, "output_tokens": 6, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "78", "question": "Three unreliable narrators share a commute and a crime spreadsheet.", "answer_ref": "Grand Theft Auto V", "aliases": ["GTA V"], "model": "azure_openai:gpt-4o", "answer": "The Girl on the Train", "latency_ms": 539, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 72, "output_tokens": 6, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "80", "question": "Interstellar HR balancing romance, diplomacy, and space bugs.", "answer_ref": "Mass Effect", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Starship Troopers", "latency_ms": 475, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "81", "question": "Short on continents, tall on ambitions and tailored coats.", "answer_ref": "Napoleon Bonaparte", "aliases": ["Napoleon"], "model": "azure_openai:gpt-4o", "answer": "Napoleon Bonaparte", "latency_ms": 529, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 6, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "83", "question": "Teenage strategist whose mail came with flames.", "answer_ref": "Joan of Arc", "aliases": ["Jeanne d'Arc"], "model": "azure_openai:gpt-4o", "answer": "Joan of Arc", "latency_ms": 485, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 5, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "76", "question": "Adoption paperwork scattered across war and alchemy.", "answer_ref": "The Witcher 3: Wild Hunt", "aliases": ["The Witcher 3"], "model": "azure_openai:gpt-4o", "answer": "Fullmetal Alchemist", "latency_ms": 971, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 70, "output_tokens": 5, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "77", "question": "Campfire ledger balances loyalty, debt, and westward entropy.", "answer_ref": "Red Dead Redemption 2", "aliases": ["RDR2"], "model": "azure_openai:gpt-4o", "answer": "Red Dead Redemption", "latency_ms": 981, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "82", "question": "Postal service enthusiast with aggressive merger policies.", "answer_ref": "Genghis Khan", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Postmaster General", "latency_ms": 593, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 68, "output_tokens": 4, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "84", "question": "Glowing research with a half-life of acclaim.", "answer_ref": "Marie Curie", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Chernobyl", "latency_ms": 511, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 70, "output_tokens": 4, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "86", "question": "One-eyed project manager who pays ravens in secrets.", "answer_ref": "Odin", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Odin", "latency_ms": 498, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "87", "question": "Strategy consultant whose owl handles night shift.", "answer_ref": "Athena", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Minerva McGonagall", "latency_ms": 623, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 68, "output_tokens": 8, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "88", "question": "Underworld landlord with a pomegranate clause.", "answer_ref": "Hades", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Hades", "latency_ms": 523, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "89", "question": "Village rooflines under a swirling insomnia.", "answer_ref": "Starry Night", "aliases": ["The Starry Night"], "model": "azure_openai:gpt-4o", "answer": "Starry Night", "latency_ms": 498, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 68, "output_tokens": 4, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "85", "question": "Hair that suggests stormy weather for equations.", "answer_ref": "Albert Einstein", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Einstein", "latency_ms": 958, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "92", "question": "Shiba evaluates life with quantifiers and Comic Sans.", "answer_ref": "Doge", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Doge", "latency_ms": 491, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "91", "question": "Allegiance wanders mid-stride; captions do the rest.", "answer_ref": "Distracted Boyfriend", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Forrest Gump", "latency_ms": 567, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 74, "output_tokens": 5, "total_tokens": 79, "reasoning_tokens": 0}
{"id": "93", "question": "Airborne accountant who tallied rings in Chicago.", "answer_ref": "Michael Jordan", "aliases": ["MJ"], "model": "azure_openai:gpt-4o", "answer": "Michael Jordan", "latency_ms": 484, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "94", "question": "Serve so decisive it files its own appeal.", "answer_ref": "Serena Williams", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Ace", "latency_ms": 484, "is_exact": false, "is_alias": false, "fuzzy": 11, "is_correct": false, "input_tokens": 69, "output_tokens": 2, "total_tokens": 71, "reasoning_tokens": 0}
{"id": "95", "question": "Finish line receives delivery before the starting gun's echo.", "answer_ref": "Usain Bolt", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Relay baton", "latency_ms": 501, "is_exact": false, "is_alias": false, "fuzzy": 38, "is_correct": false, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "96", "question": "Purple ethicist proposes resource management by finger interface.", "answer_ref": "Thanos", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Thanos", "latency_ms": 497, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "90", "question": "Clocks take a nap over a hot afternoon.", "answer_ref": "The Persistence of Memory", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Siesta Key", "latency_ms": 971, "is_exact": false, "is_alias": false, "fuzzy": 45, "is_correct": false, "input_tokens": 70, "output_tokens": 4, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "97", "question": "Patriotic discus enthusiast unfrozen for modern meetings.", "answer_ref": "Captain America", "aliases": ["Steve Rogers"], "model": "azure_openai:gpt-4o", "answer": "Captain America", "latency_ms": 500, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "98", "question": "Stand-up philosophy major specializing in citywide punchlines.", "answer_ref": "Joker", "aliases": ["The Joker"], "model": "azure_openai:gpt-4o", "answer": "George Carlin", "latency_ms": 496, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "99", "question": "Imprisonment catered by dumplings; vengeance plated cold.", "answer_ref": "Oldboy", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Oldboy", "latency_ms": 479, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "100", "question": "Typewriter productivity meets corridor geometry and mirrored warnings.", "answer_ref": "The Shining", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Shining", "latency_ms": 478, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 4, "total_tokens": 74, "reasoning_tokens": 0}
