{"id": "4", "question": "Parents who sample the buffet pay in porcine installments.", "answer_ref": "Spirited Away", "aliases": ["Sen to Chihiro no Kamikakushi"], "model": "azure_openai:gpt-5-chat", "answer": "Hamlet", "latency_ms": 1742, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "5", "question": "Two families share an address book but not the same floor.", "answer_ref": "Parasite", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Duplex", "latency_ms": 1791, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "2", "question": "Briefcase with indoor sunrise, diner philosophy, and a dance that solves nothing.", "answer_ref": "Pulp Fiction", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Pulp Fiction", "latency_ms": 1823, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 4, "total_tokens": 80, "reasoning_tokens": 0}
{"id": "1", "question": "Stone dust, chess pieces, and a poster that moonlights as a door.", "answer_ref": "The Shawshank Redemption", "aliases": ["Shawshank Redemption"], "model": "azure_openai:gpt-5-chat", "answer": "The Shawshank Redemption", "latency_ms": 1864, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 6, "total_tokens": 82, "reasoning_tokens": 0}
{"id": "7", "question": "Citywide social experiment involving ferries, coins, and pencils.", "answer_ref": "The Dark Knight", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Dark Knight", "latency_ms": 1862, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "6", "question": "Road trip powered by salt, chrome, and amplified grievances.", "answer_ref": "Mad Max: Fury Road", "aliases": ["Mad Max Fury Road"], "model": "azure_openai:gpt-5-chat", "answer": "Mad Max Fury Road", "latency_ms": 1868, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "3", "question": "Soap-funded therapy where names are withheld and noses aren't.", "answer_ref": "Fight Club", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Fight Club", "latency_ms": 1892, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "8", "question": "Farming, clocks in closets, and ocean with one extremely punctual wave.", "answer_ref": "Interstellar", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Interstellar", "latency_ms": 2038, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 3, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "9", "question": "Monolith-led fitness program for apes and spacecraft alike.", "answer_ref": "2001: A Space Odyssey", "aliases": ["2001"], "model": "azure_openai:gpt-5-chat", "answer": "2001: A Space Odyssey", "latency_ms": 354, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 7, "total_tokens": 79, "reasoning_tokens": 0}
{"id": "10", "question": "Blue-collar crew meets uninvited stowaway with dental enthusiasm.", "answer_ref": "Alien", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Alien", "latency_ms": 325, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 2, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "12", "question": "Teen's history homework graded by photographs and lightning appointments.", "answer_ref": "Back to the Future", "aliases": ["BTTF"], "model": "azure_openai:gpt-5-chat", "answer": "Flashcard", "latency_ms": 320, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "11", "question": "Reformed biker babysits future union leader against liquid competition.", "answer_ref": "Terminator 2: Judgment Day", "aliases": ["T2", "Terminator 2"], "model": "azure_openai:gpt-5-chat", "answer": "Terminator 2: Judgment Day", "latency_ms": 367, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 8, "total_tokens": 80, "reasoning_tokens": 0}
{"id": "14", "question": "Beach town practices seasonal denial until a fin writes the memo.", "answer_ref": "Jaws", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "<error: Azure has not provided the response due to a content filter being triggered>", "latency_ms": 329, "is_exact": false, "is_alias": false, "fuzzy": 5, "is_correct": false}
{"id": "15", "question": "Paperwork, pianos, and airport goodbyes rehearse wartime ethics.", "answer_ref": "Casablanca", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Casablanca", "latency_ms": 309, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 77, "output_tokens": 3, "total_tokens": 80, "reasoning_tokens": 0}
{"id": "13", "question": "Lunch runs late when the chefs turn out to be the menu.", "answer_ref": "Jurassic Park", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "<error: Azure has not provided the response due to a content filter being triggered>", "latency_ms": 363, "is_exact": false, "is_alias": false, "fuzzy": 18, "is_correct": false}
{"id": "16", "question": "Revenge business with rodents and a suspiciously literate giant.", "answer_ref": "The Princess Bride", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Princess Bride", "latency_ms": 350, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 4, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "17", "question": "Census of moths assists in culinary crime prevention.", "answer_ref": "The Silence of the Lambs", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Mothman", "latency_ms": 342, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "18", "question": "Detectives follow a cookbook with exactly seven courses.", "answer_ref": "Se7en", "aliases": ["Seven"], "model": "azure_openai:gpt-5-chat", "answer": "Seven Samurai", "latency_ms": 341, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "22", "question": "Linguistics class with non-linear attendance policy and squid notebooks.", "answer_ref": "Arrival", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Arrival", "latency_ms": 294, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "21", "question": "Operating system writes love letters when not updating itself.", "answer_ref": "Her", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Her", "latency_ms": 311, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 2, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "20", "question": "Metronome used as a weapon in a percussion war.", "answer_ref": "Whiplash", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Whiplash", "latency_ms": 337, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "23", "question": "Insurance salesman discovers the sky has customer service.", "answer_ref": "The Truman Show", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Truman Show", "latency_ms": 340, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "24", "question": "Small town trapped in a calendar bug until empathy patches it.", "answer_ref": "Groundhog Day", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Groundhog Day", "latency_ms": 343, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "25", "question": "Rug replacement escalates into bowling diplomacy.", "answer_ref": "The Big Lebowski", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Big Lebowski", "latency_ms": 344, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 5, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "19", "question": "Friendship diluted by shares, depositions, and a hoodie.", "answer_ref": "The Social Network", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Social Network", "latency_ms": 632, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "26", "question": "Coin tosses decide fate; oxygen tank enforces agreements.", "answer_ref": "No Country for Old Men", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "No Country for Old Men", "latency_ms": 370, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 6, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "29", "question": "Hidden nation negotiates outreach with feline-themed foreign policy.", "answer_ref": "Black Panther", "aliases": ["Marvel's Black Panther"], "model": "azure_openai:gpt-5-chat", "answer": "Wakanda", "latency_ms": 327, "is_exact": false, "is_alias": false, "fuzzy": 40, "is_correct": false, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "27", "question": "Milkshakes become a mineral rights seminar.", "answer_ref": "There Will Be Blood", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "There Will Be Blood", "latency_ms": 359, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 5, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "28", "question": "Lobby boy chronicles pastries, prison breaks, and purple uniforms.", "answer_ref": "The Grand Budapest Hotel", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Grand Budapest Hotel", "latency_ms": 342, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "30", "question": "Weekend retreat with tea, chairs, and unwilling houseguests.", "answer_ref": "Get Out", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Tea Party in Wonderland", "latency_ms": 335, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 73, "output_tokens": 5, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "32", "question": "One city translates the word 'system' into many dialects.", "answer_ref": "The Wire", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "New York", "latency_ms": 330, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "31", "question": "Meteorite cutlery causes HR issues across several climates.", "answer_ref": "Game of Thrones", "aliases": ["GoT"], "model": "azure_openai:gpt-5-chat", "answer": "The Hitchhiker's Guide to the Galaxy", "latency_ms": 395, "is_exact": false, "is_alias": false, "fuzzy": 37, "is_correct": false, "input_tokens": 71, "output_tokens": 10, "total_tokens": 81, "reasoning_tokens": 0}
{"id": "36", "question": "Coffeehouse residency with a couch that never graduates.", "answer_ref": "Friends", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Friends", "latency_ms": 306, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 2, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "35", "question": "Paper company proves boredom is a renewable resource.", "answer_ref": "The Office (US)", "aliases": ["The Office US"], "model": "azure_openai:gpt-5-chat", "answer": "The Office", "latency_ms": 324, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "34", "question": "Suburban bikes explore a basement that forgot its ceiling.", "answer_ref": "Stranger Things", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Stranger Things", "latency_ms": 351, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "37", "question": "A show about nothing that itemizes everything.", "answer_ref": "Seinfeld", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Seinfeld", "latency_ms": 332, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "38", "question": "Owls serve as unreliable narrators for cherry pie mysteries.", "answer_ref": "Twin Peaks", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Twin Peaks", "latency_ms": 319, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "39", "question": "Modern gadgets audition for the role of ancient curses.", "answer_ref": "Black Mirror", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Black Mirror", "latency_ms": 329, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "40", "question": "Space western where childcare doubles as payroll.", "answer_ref": "The Mandalorian", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Mandalorian", "latency_ms": 323, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 68, "output_tokens": 5, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "41", "question": "Nocturnal philanthropist with cave-based commuting solutions.", "answer_ref": "Batman", "aliases": ["Bruce Wayne"], "model": "azure_openai:gpt-5-chat", "answer": "Batman", "latency_ms": 297, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 2, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "42", "question": "Freelance photographer with sticky commute and precognitive tingles.", "answer_ref": "Spider-Man", "aliases": ["Peter Parker", "Spiderman"], "model": "azure_openai:gpt-5-chat", "answer": "Spider-Man", "latency_ms": 318, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "43", "question": "Cigar aficionado with cutlery you can't confiscate.", "answer_ref": "Wolverine", "aliases": ["Logan"], "model": "azure_openai:gpt-5-chat", "answer": "Wolverine", "latency_ms": 340, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "44", "question": "Traveling consultant whose product demos explode on contact.", "answer_ref": "James Bond", "aliases": ["007"], "model": "azure_openai:gpt-5-chat", "answer": "Mission Impossible", "latency_ms": 349, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "45", "question": "Archaeology performed with greater emphasis on cardio than citations.", "answer_ref": "Lara Croft", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Indiana Jones", "latency_ms": 327, "is_exact": false, "is_alias": false, "fuzzy": 17, "is_correct": false, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "46", "question": "Green armored introvert who solves conflicts by respawning less.", "answer_ref": "Master Chief", "aliases": ["John-117"], "model": "azure_openai:gpt-5-chat", "answer": "Master Chief", "latency_ms": 331, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "49", "question": "Silent handyman who fixes kingdoms by breaking pots.", "answer_ref": "Link", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Link", "latency_ms": 301, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 2, "total_tokens": 71, "reasoning_tokens": 0}
{"id": "47", "question": "Fatherhood meets anger management with mythic collateral.", "answer_ref": "Kratos", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "God of War", "latency_ms": 346, "is_exact": false, "is_alias": false, "fuzzy": 12, "is_correct": false, "input_tokens": 70, "output_tokens": 4, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "48", "question": "Freelance pest control for morally ambiguous fairy tales.", "answer_ref": "Geralt of Rivia", "aliases": ["Geralt"], "model": "azure_openai:gpt-5-chat", "answer": "Rumpelstiltskin", "latency_ms": 345, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 71, "output_tokens": 7, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "33", "question": "Therapy sessions where patient confidentiality competes with RICO.", "answer_ref": "The Sopranos", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Sopranos", "latency_ms": 992, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "50", "question": "Engineer whose wearable midlife crisis saves the world.", "answer_ref": "Tony Stark", "aliases": ["Iron Man", "Tony Stark"], "model": "azure_openai:gpt-5-chat", "answer": "Iron Man", "latency_ms": 314, "is_exact": false, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "51", "question": "Stick with an HR policy against retirement.", "answer_ref": "Elder Wand", "aliases": ["Deathstick"], "model": "azure_openai:gpt-5-chat", "answer": "Peter Pan", "latency_ms": 319, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 68, "output_tokens": 3, "total_tokens": 71, "reasoning_tokens": 0}
{"id": "52", "question": "Glove that treats population as a configurable setting.", "answer_ref": "Infinity Gauntlet", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Infinity Gauntlet", "latency_ms": 333, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 5, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "53", "question": "Tri-pronged upgrade that lets cars miss deadlines.", "answer_ref": "Flux Capacitor", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Trident of Poseidon", "latency_ms": 353, "is_exact": false, "is_alias": false, "fuzzy": 24, "is_correct": false, "input_tokens": 70, "output_tokens": 6, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "54", "question": "Oversized cutlery popular with gravity skeptics in Midgar.", "answer_ref": "Buster Sword", "aliases": ["Cloud's sword"], "model": "azure_openai:gpt-5-chat", "answer": "Buster Sword", "latency_ms": 335, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "56", "question": "Weighted friendship that fails basic combustion tests.", "answer_ref": "Companion Cube", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Companion Cube", "latency_ms": 332, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 68, "output_tokens": 4, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "55", "question": "Handheld shortcut that turns floors into suggestions.", "answer_ref": "Portal Gun", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Magic carpet", "latency_ms": 337, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "58", "question": "Urban nightlight attracts themed criminals like moths.", "answer_ref": "Gotham City", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Gotham City", "latency_ms": 358, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 5, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "59", "question": "Anytown USA with a five-finger discount on anatomy.", "answer_ref": "Springfield", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Body Snatchers", "latency_ms": 350, "is_exact": false, "is_alias": false, "fuzzy": 24, "is_correct": false, "input_tokens": 73, "output_tokens": 5, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "57", "question": "Municipal planning by pharmaceutical committee goes poorly.", "answer_ref": "Raccoon City", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "City of Angels", "latency_ms": 430, "is_exact": false, "is_alias": false, "fuzzy": 53, "is_correct": false, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "60", "question": "Mountain resort with a generous winter rate for ghosts.", "answer_ref": "The Overlook Hotel", "aliases": ["Overlook Hotel"], "model": "azure_openai:gpt-5-chat", "answer": "The Overlook Hotel", "latency_ms": 373, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 5, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "64", "question": "Teen critiques phonies while auditioning for solitude.", "answer_ref": "The Catcher in the Rye", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "<error: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'high'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}>", "latency_ms": 289, "is_exact": false, "is_alias": false, "fuzzy": 6, "is_correct": false}
{"id": "62", "question": "Dry world where rhythms of sand set politics and prophecy.", "answer_ref": "Dune", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Dune", "latency_ms": 314, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "61", "question": "Dictionary on a diet and screens that watch back.", "answer_ref": "1984", "aliases": ["Nineteen Eighty-Four"], "model": "azure_openai:gpt-5-chat", "answer": "Smartphone", "latency_ms": 322, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "63", "question": "Unexpected travel invoice itemized by songs, maps, and barrels.", "answer_ref": "The Hobbit", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Pirates of the Caribbean", "latency_ms": 364, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 73, "output_tokens": 6, "total_tokens": 79, "reasoning_tokens": 0}
{"id": "65", "question": "Musician studies physics to pronounce reality correctly.", "answer_ref": "The Name of the Wind", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Brian May", "latency_ms": 338, "is_exact": false, "is_alias": false, "fuzzy": 19, "is_correct": false, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "66", "question": "Opera, rock, and confession crammed into one late-night phone call.", "answer_ref": "Bohemian Rhapsody", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Bohemian Rhapsody", "latency_ms": 357, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 6, "total_tokens": 81, "reasoning_tokens": 0}
{"id": "68", "question": "Promise-heavy synth pledge weaponized by surprise.", "answer_ref": "Never Gonna Give You Up", "aliases": ["Rickroll song"], "model": "azure_openai:gpt-5-chat", "answer": "Blade Runner", "latency_ms": 315, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "67", "question": "Red jacket, moonwalk, and a zombie rehearsal dinner.", "answer_ref": "Thriller", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Michael Jackson", "latency_ms": 350, "is_exact": false, "is_alias": false, "fuzzy": 17, "is_correct": false, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "69", "question": "Water metaphors filed under overdue heartbreak invoices.", "answer_ref": "Rolling in the Deep", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Sea of Tears", "latency_ms": 344, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "71", "question": "Falling real estate teaches ruthless minimalism.", "answer_ref": "Tetris", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Tetris", "latency_ms": 344, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "72", "question": "Cubist wilderness where punching trees becomes architecture.", "answer_ref": "Minecraft", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Minecraft", "latency_ms": 303, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 2, "total_tokens": 71, "reasoning_tokens": 0}
{"id": "70", "question": "Horse-free riding lesson conducted in a tux.", "answer_ref": "Gangnam Style", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Ballroom dancing", "latency_ms": 356, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "73", "question": "Space crew plays HR's least favorite trust exercise.", "answer_ref": "Among Us", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Among Us", "latency_ms": 321, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "75", "question": "Death-as-syllabus with bonfires for office hours.", "answer_ref": "Dark Souls", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "<error: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}>", "latency_ms": 284, "is_exact": false, "is_alias": false, "fuzzy": 4, "is_correct": false}
{"id": "74", "question": "Open-world archaeology of despair, punctuated by grace checkpoints.", "answer_ref": "Elden Ring", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Elden Ring", "latency_ms": 327, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 4, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "76", "question": "Adoption paperwork scattered across war and alchemy.", "answer_ref": "The Witcher 3: Wild Hunt", "aliases": ["The Witcher 3"], "model": "azure_openai:gpt-5-chat", "answer": "Fullmetal Alchemist", "latency_ms": 365, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 70, "output_tokens": 5, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "77", "question": "Campfire ledger balances loyalty, debt, and westward entropy.", "answer_ref": "Red Dead Redemption 2", "aliases": ["RDR2"], "model": "azure_openai:gpt-5-chat", "answer": "Red Dead Redemption", "latency_ms": 324, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "80", "question": "Interstellar HR balancing romance, diplomacy, and space bugs.", "answer_ref": "Mass Effect", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Mass Effect", "latency_ms": 308, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 3, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "79", "question": "Science fair judged by cake lore and lemons.", "answer_ref": "Portal 2", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Marie Curie", "latency_ms": 316, "is_exact": false, "is_alias": false, "fuzzy": 21, "is_correct": false, "input_tokens": 69, "output_tokens": 4, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "78", "question": "Three unreliable narrators share a commute and a crime spreadsheet.", "answer_ref": "Grand Theft Auto V", "aliases": ["GTA V"], "model": "azure_openai:gpt-5-chat", "answer": "The Girl on the Train", "latency_ms": 348, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 72, "output_tokens": 6, "total_tokens": 78, "reasoning_tokens": 0}
{"id": "82", "question": "Postal service enthusiast with aggressive merger policies.", "answer_ref": "Genghis Khan", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Postmaster General", "latency_ms": 322, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 68, "output_tokens": 4, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "81", "question": "Short on continents, tall on ambitions and tailored coats.", "answer_ref": "Napoleon Bonaparte", "aliases": ["Napoleon"], "model": "azure_openai:gpt-5-chat", "answer": "Napoleon Bonaparte", "latency_ms": 339, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 6, "total_tokens": 77, "reasoning_tokens": 0}
{"id": "83", "question": "Teenage strategist whose mail came with flames.", "answer_ref": "Joan of Arc", "aliases": ["Jeanne d'Arc"], "model": "azure_openai:gpt-5-chat", "answer": "Joan of Arc", "latency_ms": 330, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 5, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "84", "question": "Glowing research with a half-life of acclaim.", "answer_ref": "Marie Curie", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Marie Curie", "latency_ms": 333, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 4, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "85", "question": "Hair that suggests stormy weather for equations.", "answer_ref": "Albert Einstein", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Einstein", "latency_ms": 307, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "86", "question": "One-eyed project manager who pays ravens in secrets.", "answer_ref": "Odin", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Odin", "latency_ms": 304, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "87", "question": "Strategy consultant whose owl handles night shift.", "answer_ref": "Athena", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Athena", "latency_ms": 356, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 68, "output_tokens": 3, "total_tokens": 71, "reasoning_tokens": 0}
{"id": "88", "question": "Underworld landlord with a pomegranate clause.", "answer_ref": "Hades", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Hades", "latency_ms": 341, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "90", "question": "Clocks take a nap over a hot afternoon.", "answer_ref": "The Persistence of Memory", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Siesta", "latency_ms": 316, "is_exact": false, "is_alias": false, "fuzzy": 29, "is_correct": false, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "89", "question": "Village rooflines under a swirling insomnia.", "answer_ref": "Starry Night", "aliases": ["The Starry Night"], "model": "azure_openai:gpt-5-chat", "answer": "The Starry Night", "latency_ms": 353, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 68, "output_tokens": 5, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "91", "question": "Allegiance wanders mid-stride; captions do the rest.", "answer_ref": "Distracted Boyfriend", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Forrest Gump", "latency_ms": 317, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 74, "output_tokens": 5, "total_tokens": 79, "reasoning_tokens": 0}
{"id": "92", "question": "Shiba evaluates life with quantifiers and Comic Sans.", "answer_ref": "Doge", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Doge meme", "latency_ms": 348, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 4, "total_tokens": 75, "reasoning_tokens": 0}
{"id": "94", "question": "Serve so decisive it files its own appeal.", "answer_ref": "Serena Williams", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Ace Attorney", "latency_ms": 301, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false, "input_tokens": 69, "output_tokens": 3, "total_tokens": 72, "reasoning_tokens": 0}
{"id": "93", "question": "Airborne accountant who tallied rings in Chicago.", "answer_ref": "Michael Jordan", "aliases": ["MJ"], "model": "azure_openai:gpt-5-chat", "answer": "Michael Jordan", "latency_ms": 336, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "95", "question": "Finish line receives delivery before the starting gun's echo.", "answer_ref": "Usain Bolt", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Back to the Future", "latency_ms": 330, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 71, "output_tokens": 5, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "96", "question": "Purple ethicist proposes resource management by finger interface.", "answer_ref": "Thanos", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Thanos", "latency_ms": 323, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 3, "total_tokens": 73, "reasoning_tokens": 0}
{"id": "97", "question": "Patriotic discus enthusiast unfrozen for modern meetings.", "answer_ref": "Captain America", "aliases": ["Steve Rogers"], "model": "azure_openai:gpt-5-chat", "answer": "Captain America", "latency_ms": 321, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74, "reasoning_tokens": 0}
{"id": "99", "question": "Imprisonment catered by dumplings; vengeance plated cold.", "answer_ref": "Oldboy", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "Oldboy", "latency_ms": 311, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 3, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "98", "question": "Stand-up philosophy major specializing in citywide punchlines.", "answer_ref": "Joker", "aliases": ["The Joker"], "model": "azure_openai:gpt-5-chat", "answer": "Gotham Joker", "latency_ms": 348, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 5, "total_tokens": 76, "reasoning_tokens": 0}
{"id": "100", "question": "Typewriter productivity meets corridor geometry and mirrored warnings.", "answer_ref": "The Shining", "aliases": [], "model": "azure_openai:gpt-5-chat", "answer": "The Shining", "latency_ms": 330, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 4, "total_tokens": 74, "reasoning_tokens": 0}
