{"id": "2", "question": "Briefcase with indoor sunrise, diner philosophy, and a dance that solves nothing.", "answer_ref": "Pulp Fiction", "aliases": [], "model": "azure_openai:o3-pro", "answer": "Pulp Fiction", "latency_ms": 17861, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 329, "total_tokens": 404, "reasoning_tokens": 320}
{"id": "6", "question": "Road trip powered by salt, chrome, and amplified grievances.", "answer_ref": "Mad Max: Fury Road", "aliases": ["Mad Max Fury Road"], "model": "azure_openai:o3-pro", "answer": "Mad Max: Fury Road", "latency_ms": 35813, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 139, "total_tokens": 210, "reasoning_tokens": 128}
{"id": "7", "question": "Citywide social experiment involving ferries, coins, and pencils.", "answer_ref": "The Dark Knight", "aliases": [], "model": "azure_openai:o3-pro", "answer": "The Dark Knight", "latency_ms": 36084, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 201, "total_tokens": 273, "reasoning_tokens": 192}
{"id": "3", "question": "Soap-funded therapy where names are withheld and noses aren't.", "answer_ref": "Fight Club", "aliases": [], "model": "azure_openai:o3-pro", "answer": "Fight Club", "latency_ms": 41336, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 70, "output_tokens": 648, "total_tokens": 718, "reasoning_tokens": 640}
{"id": "10", "question": "Blue-collar crew meets uninvited stowaway with dental enthusiasm.", "answer_ref": "Alien", "aliases": [], "model": "azure_openai:o3-pro", "answer": "Alien", "latency_ms": 45162, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 391, "total_tokens": 465, "reasoning_tokens": 384}
{"id": "1", "question": "Stone dust, chess pieces, and a poster that moonlights as a door.", "answer_ref": "The Shawshank Redemption", "aliases": ["Shawshank Redemption"], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 183294, "is_exact": false, "is_alias": false, "fuzzy": 23, "is_correct": false}
{"id": "5", "question": "Two families share an address book but not the same floor.", "answer_ref": "Parasite", "aliases": [], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 183294, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false}
{"id": "4", "question": "Parents who sample the buffet pay in porcine installments.", "answer_ref": "Spirited Away", "aliases": ["Sen to Chihiro no Kamikakushi"], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 183381, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false}
{"id": "8", "question": "Farming, clocks in closets, and ocean with one extremely punctual wave.", "answer_ref": "Interstellar", "aliases": [], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 183415, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false}
{"id": "9", "question": "Monolith-led fitness program for apes and spacecraft alike.", "answer_ref": "2001: A Space Odyssey", "aliases": ["2001"], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 182014, "is_exact": false, "is_alias": false, "fuzzy": 24, "is_correct": false}
{"id": "11", "question": "Reformed biker babysits future union leader against liquid competition.", "answer_ref": "Terminator 2: Judgment Day", "aliases": ["T2", "Terminator 2"], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 181923, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false}
{"id": "12", "question": "Teen's history homework graded by photographs and lightning appointments.", "answer_ref": "Back to the Future", "aliases": ["BTTF"], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 181907, "is_exact": false, "is_alias": false, "fuzzy": 37, "is_correct": false}
{"id": "14", "question": "Beach town practices seasonal denial until a fin writes the memo.", "answer_ref": "Jaws", "aliases": [], "model": "azure_openai:o3-pro", "answer": "Jaws", "latency_ms": 47388, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 456, "total_tokens": 527, "reasoning_tokens": 448}
{"id": "15", "question": "Paperwork, pianos, and airport goodbyes rehearse wartime ethics.", "answer_ref": "Casablanca", "aliases": [], "model": "azure_openai:o3-pro", "answer": "Casablanca", "latency_ms": 56070, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 264, "total_tokens": 340, "reasoning_tokens": 256}
{"id": "18", "question": "Detectives follow a cookbook with exactly seven courses.", "answer_ref": "Se7en", "aliases": ["Seven"], "model": "azure_openai:o3-pro", "answer": "Se7en", "latency_ms": 58597, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 69, "output_tokens": 521, "total_tokens": 590, "reasoning_tokens": 512}
{"id": "13", "question": "Lunch runs late when the chefs turn out to be the menu.", "answer_ref": "Jurassic Park", "aliases": [], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 181950, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false}
{"id": "22", "question": "Linguistics class with non-linear attendance policy and squid notebooks.", "answer_ref": "Arrival", "aliases": [], "model": "azure_openai:o3-pro", "answer": "Arrival", "latency_ms": 47756, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 72, "output_tokens": 263, "total_tokens": 335, "reasoning_tokens": 256}
{"id": "24", "question": "Small town trapped in a calendar bug until empathy patches it.", "answer_ref": "Groundhog Day", "aliases": [], "model": "azure_openai:o3-pro", "answer": "Groundhog Day", "latency_ms": 41528, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 201, "total_tokens": 272, "reasoning_tokens": 192}
{"id": "25", "question": "Rug replacement escalates into bowling diplomacy.", "answer_ref": "The Big Lebowski", "aliases": [], "model": "azure_openai:o3-pro", "answer": "The Big Lebowski", "latency_ms": 56071, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 68, "output_tokens": 330, "total_tokens": 398, "reasoning_tokens": 320}
{"id": "26", "question": "Coin tosses decide fate; oxygen tank enforces agreements.", "answer_ref": "No Country for Old Men", "aliases": [], "model": "azure_openai:o3-pro", "answer": "Anton Chigurh", "latency_ms": 54751, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 71, "output_tokens": 202, "total_tokens": 273, "reasoning_tokens": 192}
{"id": "17", "question": "Census of moths assists in culinary crime prevention.", "answer_ref": "The Silence of the Lambs", "aliases": [], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 182313, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false}
{"id": "16", "question": "Revenge business with rodents and a suspiciously literate giant.", "answer_ref": "The Princess Bride", "aliases": [], "model": "azure_openai:o3-pro", "answer": "<error: Request timed out.>", "latency_ms": 182557, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false}
