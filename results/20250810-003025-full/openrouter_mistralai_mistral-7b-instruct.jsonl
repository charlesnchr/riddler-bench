{"id": "3", "question": "Soap-funded therapy where names are withheld and noses aren't.", "answer_ref": "Fight Club", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Alcoholics Anonymous (AA)", "latency_ms": 1171, "is_exact": false, "is_alias": false, "fuzzy": 18, "is_correct": false, "input_tokens": 80, "output_tokens": 8, "total_tokens": 88}
{"id": "1", "question": "Stone dust, chess pieces, and a poster that moonlights as a door.", "answer_ref": "The Shawshank Redemption", "aliases": ["Shawshank Redemption"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Mona Lisa (The Louvre)", "latency_ms": 1247, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 80, "output_tokens": 8, "total_tokens": 88}
{"id": "5", "question": "Two families share an address book but not the same floor.", "answer_ref": "Parasite", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Empire State Building", "latency_ms": 1246, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 75, "output_tokens": 4, "total_tokens": 79}
{"id": "4", "question": "Parents who sample the buffet pay in porcine installments.", "answer_ref": "Spirited Away", "aliases": ["Sen to Chihiro no Kamikakushi"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The Bank of Montreal (BMO)", "latency_ms": 1256, "is_exact": false, "is_alias": false, "fuzzy": 32, "is_correct": false, "input_tokens": 78, "output_tokens": 9, "total_tokens": 87}
{"id": "2", "question": "Briefcase with indoor sunrise, diner philosophy, and a dance that solves nothing.", "answer_ref": "Pulp Fiction", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "\"Memento\" (movie)", "latency_ms": 2075, "is_exact": false, "is_alias": false, "fuzzy": 24, "is_correct": false, "input_tokens": 82, "output_tokens": 9, "total_tokens": 91}
{"id": "8", "question": "Farming, clocks in closets, and ocean with one extremely punctual wave.", "answer_ref": "Interstellar", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Little Mermaid", "latency_ms": 863, "is_exact": false, "is_alias": false, "fuzzy": 38, "is_correct": false, "input_tokens": 81, "output_tokens": 4, "total_tokens": 85}
{"id": "6", "question": "Road trip powered by salt, chrome, and amplified grievances.", "answer_ref": "Mad Max: Fury Road", "aliases": ["Mad Max Fury Road"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Cross Country Cadillac", "latency_ms": 985, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 80, "output_tokens": 6, "total_tokens": 86}
{"id": "7", "question": "Citywide social experiment involving ferries, coins, and pencils.", "answer_ref": "The Dark Knight", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Monopoly", "latency_ms": 1262, "is_exact": false, "is_alias": false, "fuzzy": 10, "is_correct": false, "input_tokens": 78, "output_tokens": 4, "total_tokens": 82}
{"id": "9", "question": "Monolith-led fitness program for apes and spacecraft alike.", "answer_ref": "2001: A Space Odyssey", "aliases": ["2001"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Planet Fitness", "latency_ms": 1320, "is_exact": false, "is_alias": false, "fuzzy": 37, "is_correct": false, "input_tokens": 78, "output_tokens": 5, "total_tokens": 83}
{"id": "11", "question": "Reformed biker babysits future union leader against liquid competition.", "answer_ref": "Terminator 2: Judgment Day", "aliases": ["T2", "Terminator 2"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Harley Davidson Union Leader (Sly Stallone character in the movie \"Rocky Balboa\")", "latency_ms": 1005, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 76, "output_tokens": 24, "total_tokens": 100}
{"id": "10", "question": "Blue-collar crew meets uninvited stowaway with dental enthusiasm.", "answer_ref": "Alien", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Titanic (movie)", "latency_ms": 1262, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 79, "output_tokens": 8, "total_tokens": 87}
{"id": "12", "question": "Teen's history homework graded by photographs and lightning appointments.", "answer_ref": "Back to the Future", "aliases": ["BTTF"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Library of Congress", "latency_ms": 1280, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 78, "output_tokens": 4, "total_tokens": 82}
{"id": "14", "question": "Beach town practices seasonal denial until a fin writes the memo.", "answer_ref": "Jaws", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Monterey", "latency_ms": 975, "is_exact": false, "is_alias": false, "fuzzy": 0, "is_correct": false, "input_tokens": 77, "output_tokens": 3, "total_tokens": 80}
{"id": "13", "question": "Lunch runs late when the chefs turn out to be the menu.", "answer_ref": "Jurassic Park", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "\"Alice in Wonderland\" (Character)", "latency_ms": 1131, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 78, "output_tokens": 11, "total_tokens": 89}
{"id": "15", "question": "Paperwork, pianos, and airport goodbyes rehearse wartime ethics.", "answer_ref": "Casablanca", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Magdalene (St. Mary's Airport, Alderney, Channel Islands)", "latency_ms": 965, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false, "input_tokens": 82, "output_tokens": 18, "total_tokens": 100}
{"id": "17", "question": "Census of moths assists in culinary crime prevention.", "answer_ref": "The Silence of the Lambs", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Food and Drug Administration (FDA)", "latency_ms": 961, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 76, "output_tokens": 8, "total_tokens": 84}
{"id": "16", "question": "Revenge business with rodents and a suspiciously literate giant.", "answer_ref": "The Princess Bride", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Ratatouille", "latency_ms": 1228, "is_exact": false, "is_alias": false, "fuzzy": 24, "is_correct": false, "input_tokens": 77, "output_tokens": 5, "total_tokens": 82}
{"id": "18", "question": "Detectives follow a cookbook with exactly seven courses.", "answer_ref": "Se7en", "aliases": ["Seven"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Seven-Up (movie or character name not found, could refer to a cookbook title or a brand of soft drink)", "latency_ms": 1091, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 26, "total_tokens": 100}
{"id": "19", "question": "Friendship diluted by shares, depositions, and a hoodie.", "answer_ref": "The Social Network", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Godfather (movie character)", "latency_ms": 1135, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 78, "output_tokens": 7, "total_tokens": 85}
{"id": "20", "question": "Metronome used as a weapon in a percussion war.", "answer_ref": "Whiplash", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "conga-war-drum", "latency_ms": 905, "is_exact": false, "is_alias": false, "fuzzy": 18, "is_correct": false, "input_tokens": 75, "output_tokens": 7, "total_tokens": 82}
{"id": "21", "question": "Operating system writes love letters when not updating itself.", "answer_ref": "Her", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Microsoft Windows", "latency_ms": 867, "is_exact": false, "is_alias": false, "fuzzy": 10, "is_correct": false, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "23", "question": "Insurance salesman discovers the sky has customer service.", "answer_ref": "The Truman Show", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Blue Cross (Blue Cross Blue Shield is a major health insurance company)", "latency_ms": 1068, "is_exact": false, "is_alias": false, "fuzzy": 19, "is_correct": false, "input_tokens": 74, "output_tokens": 15, "total_tokens": 89}
{"id": "22", "question": "Linguistics class with non-linear attendance policy and squid notebooks.", "answer_ref": "Arrival", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Monty Python and the Holy Grail (movie)", "latency_ms": 1230, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 79, "output_tokens": 13, "total_tokens": 92}
{"id": "25", "question": "Rug replacement escalates into bowling diplomacy.", "answer_ref": "The Big Lebowski", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The Big Lebowski", "latency_ms": 850, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77}
{"id": "24", "question": "Small town trapped in a calendar bug until empathy patches it.", "answer_ref": "Groundhog Day", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Groundhog Day (movie)", "latency_ms": 1667, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 76, "output_tokens": 9, "total_tokens": 85}
{"id": "27", "question": "Milkshakes become a mineral rights seminar.", "answer_ref": "There Will Be Blood", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Oreo (Cookie)", "latency_ms": 1090, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 74, "output_tokens": 7, "total_tokens": 81}
{"id": "26", "question": "Coin tosses decide fate; oxygen tank enforces agreements.", "answer_ref": "No Country for Old Men", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Oxygen Tank (Scuba Regulator)", "latency_ms": 1730, "is_exact": false, "is_alias": false, "fuzzy": 32, "is_correct": false, "input_tokens": 77, "output_tokens": 11, "total_tokens": 88}
{"id": "29", "question": "Hidden nation negotiates outreach with feline-themed foreign policy.", "answer_ref": "Black Panther", "aliases": ["Marvel's Black Panther"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "United Arab Emirates (UAE)", "latency_ms": 1211, "is_exact": false, "is_alias": false, "fuzzy": 43, "is_correct": false, "input_tokens": 78, "output_tokens": 10, "total_tokens": 88}
{"id": "28", "question": "Lobby boy chronicles pastries, prison breaks, and purple uniforms.", "answer_ref": "The Grand Budapest Hotel", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The Shawshank Redemption (movie)", "latency_ms": 1304, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 79, "output_tokens": 12, "total_tokens": 91}
{"id": "30", "question": "Weekend retreat with tea, chairs, and unwilling houseguests.", "answer_ref": "Get Out", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Titanic", "latency_ms": 1259, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 78, "output_tokens": 4, "total_tokens": 82}
{"id": "31", "question": "Meteorite cutlery causes HR issues across several climates.", "answer_ref": "Game of Thrones", "aliases": ["GoT"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Moon (Moon rock)", "latency_ms": 1187, "is_exact": false, "is_alias": false, "fuzzy": 41, "is_correct": false, "input_tokens": 78, "output_tokens": 7, "total_tokens": 85}
{"id": "32", "question": "One city translates the word 'system' into many dialects.", "answer_ref": "The Wire", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "United Nations (City is a metaphorical representation of the organization, not a specific city)", "latency_ms": 1011, "is_exact": false, "is_alias": false, "fuzzy": 7, "is_correct": false, "input_tokens": 76, "output_tokens": 19, "total_tokens": 95}
{"id": "33", "question": "Therapy sessions where patient confidentiality competes with RICO.", "answer_ref": "The Sopranos", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "FBI (Federal Bureau of Investigation) sessions, where patient confidentiality can potentially conflict with RICO (Racketeer Influenced and Corrupt Organizations Act) investigations.", "latency_ms": 1130, "is_exact": false, "is_alias": false, "fuzzy": 7, "is_correct": false, "input_tokens": 77, "output_tokens": 40, "total_tokens": 117}
{"id": "34", "question": "Suburban bikes explore a basement that forgot its ceiling.", "answer_ref": "Stranger Things", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Staircase (The suburban bikes are likely bicycles, and they explore the staircase by moving up and down its steps, with the \"basement\" being the lower part of the staircase and the \"ceiling\" being the top part that is often forgotten)", "latency_ms": 1207, "is_exact": false, "is_alias": false, "fuzzy": 13, "is_correct": false, "input_tokens": 75, "output_tokens": 57, "total_tokens": 132}
{"id": "37", "question": "A show about nothing that itemizes everything.", "answer_ref": "Seinfeld", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Seinfeld", "latency_ms": 842, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 71, "output_tokens": 3, "total_tokens": 74}
{"id": "35", "question": "Paper company proves boredom is a renewable resource.", "answer_ref": "The Office (US)", "aliases": ["The Office US"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Post-it Notes (3M)", "latency_ms": 1242, "is_exact": false, "is_alias": false, "fuzzy": 32, "is_correct": false, "input_tokens": 75, "output_tokens": 9, "total_tokens": 84}
{"id": "36", "question": "Coffeehouse residency with a couch that never graduates.", "answer_ref": "Friends", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Starbucks Headquarters", "latency_ms": 1076, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 77, "output_tokens": 6, "total_tokens": 83}
{"id": "39", "question": "Modern gadgets audition for the role of ancient curses.", "answer_ref": "Black Mirror", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Exorcism Device (Ouija Board)", "latency_ms": 1158, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 78, "output_tokens": 12, "total_tokens": 90}
{"id": "40", "question": "Space western where childcare doubles as payroll.", "answer_ref": "The Mandalorian", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The Matrix (movie)", "latency_ms": 910, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 74, "output_tokens": 7, "total_tokens": 81}
{"id": "38", "question": "Owls serve as unreliable narrators for cherry pie mysteries.", "answer_ref": "Twin Peaks", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Red-breasted Murder (The name of a group of crows, often used metaphorically in the novel \"Gone Girl\" by Gillian Flynn, where the character Nick Dunne is compared to an unreliable narrator, and the book includes a scene involving a cherry pie.)", "latency_ms": 1782, "is_exact": false, "is_alias": false, "fuzzy": 7, "is_correct": false, "input_tokens": 80, "output_tokens": 64, "total_tokens": 144}
{"id": "41", "question": "Nocturnal philanthropist with cave-based commuting solutions.", "answer_ref": "Batman", "aliases": ["Bruce Wayne"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Batman (Batcave)", "latency_ms": 1097, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 79, "output_tokens": 9, "total_tokens": 88}
{"id": "42", "question": "Freelance photographer with sticky commute and precognitive tingles.", "answer_ref": "Spider-Man", "aliases": ["Peter Parker", "Spiderman"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Subway (New York City)", "latency_ms": 1079, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 80, "output_tokens": 7, "total_tokens": 87}
{"id": "43", "question": "Cigar aficionado with cutlery you can't confiscate.", "answer_ref": "Wolverine", "aliases": ["Logan"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Al Capone", "latency_ms": 866, "is_exact": false, "is_alias": false, "fuzzy": 42, "is_correct": false, "input_tokens": 80, "output_tokens": 3, "total_tokens": 83}
{"id": "45", "question": "Archaeology performed with greater emphasis on cardio than citations.", "answer_ref": "Lara Croft", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Indiana Jones", "latency_ms": 829, "is_exact": false, "is_alias": false, "fuzzy": 17, "is_correct": false, "input_tokens": 76, "output_tokens": 2, "total_tokens": 78}
{"id": "47", "question": "Fatherhood meets anger management with mythic collateral.", "answer_ref": "Kratos", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Godfather", "latency_ms": 804, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false, "input_tokens": 74, "output_tokens": 2, "total_tokens": 76}
{"id": "48", "question": "Freelance pest control for morally ambiguous fairy tales.", "answer_ref": "Geralt of Rivia", "aliases": ["Geralt"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Grimm Brothers", "latency_ms": 1021, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 78, "output_tokens": 3, "total_tokens": 81}
{"id": "49", "question": "Silent handyman who fixes kingdoms by breaking pots.", "answer_ref": "Link", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Greenhorn (from the movie \"The Princess Bride\")", "latency_ms": 921, "is_exact": false, "is_alias": false, "fuzzy": 10, "is_correct": false, "input_tokens": 76, "output_tokens": 12, "total_tokens": 88}
{"id": "52", "question": "Glove that treats population as a configurable setting.", "answer_ref": "Infinity Gauntlet", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Population Control Device (a fictional device from the movie \"The Fifth Element\")", "latency_ms": 1536, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 73, "output_tokens": 17, "total_tokens": 90}
{"id": "50", "question": "Engineer whose wearable midlife crisis saves the world.", "answer_ref": "Tony Stark", "aliases": ["Iron Man", "Tony Stark"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Iron Man", "latency_ms": 3025, "is_exact": false, "is_alias": true, "fuzzy": 100, "is_correct": true, "input_tokens": 75, "output_tokens": 3, "total_tokens": 78}
{"id": "46", "question": "Green armored introvert who solves conflicts by respawning less.", "answer_ref": "Master Chief", "aliases": ["John-117"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Wolverine (from X-Men)", "latency_ms": 4051, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 78, "output_tokens": 10, "total_tokens": 88}
{"id": "53", "question": "Tri-pronged upgrade that lets cars miss deadlines.", "answer_ref": "Flux Capacitor", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "TrafficLight", "latency_ms": 848, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false, "input_tokens": 75, "output_tokens": 3, "total_tokens": 78}
{"id": "44", "question": "Traveling consultant whose product demos explode on contact.", "answer_ref": "James Bond", "aliases": ["007"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Chesley \"Sully\" Sullenberger", "latency_ms": 4340, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false, "input_tokens": 75, "output_tokens": 11, "total_tokens": 86}
{"id": "51", "question": "Stick with an HR policy against retirement.", "answer_ref": "Elder Wand", "aliases": ["Deathstick"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Social Security Administration (U.S. government agency)", "latency_ms": 2769, "is_exact": false, "is_alias": false, "fuzzy": 19, "is_correct": false, "input_tokens": 72, "output_tokens": 12, "total_tokens": 84}
{"id": "56", "question": "Weighted friendship that fails basic combustion tests.", "answer_ref": "Companion Cube", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Water (Friendship bracelet)", "latency_ms": 683, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 73, "output_tokens": 7, "total_tokens": 80}
{"id": "55", "question": "Handheld shortcut that turns floors into suggestions.", "answer_ref": "Portal Gun", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Elevator (control panel)", "latency_ms": 1008, "is_exact": false, "is_alias": false, "fuzzy": 37, "is_correct": false, "input_tokens": 72, "output_tokens": 7, "total_tokens": 79}
{"id": "58", "question": "Urban nightlight attracts themed criminals like moths.", "answer_ref": "Gotham City", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Broadway", "latency_ms": 998, "is_exact": false, "is_alias": false, "fuzzy": 21, "is_correct": false, "input_tokens": 78, "output_tokens": 1, "total_tokens": 79}
{"id": "54", "question": "Oversized cutlery popular with gravity skeptics in Midgar.", "answer_ref": "Buster Sword", "aliases": ["Cloud's sword"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "BigSpoon (from Final Fantasy VII, character Barret's large gun)", "latency_ms": 1157, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 78, "output_tokens": 18, "total_tokens": 96}
{"id": "57", "question": "Municipal planning by pharmaceutical committee goes poorly.", "answer_ref": "Raccoon City", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "City Council Pharmacy Board", "latency_ms": 1127, "is_exact": false, "is_alias": false, "fuzzy": 50, "is_correct": false, "input_tokens": 75, "output_tokens": 7, "total_tokens": 82}
{"id": "59", "question": "Anytown USA with a five-finger discount on anatomy.", "answer_ref": "Springfield", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Museum of Natural History (New York City)", "latency_ms": 972, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false, "input_tokens": 77, "output_tokens": 9, "total_tokens": 86}
{"id": "61", "question": "Dictionary on a diet and screens that watch back.", "answer_ref": "1984", "aliases": ["Nineteen Eighty-Four"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Google Nexus tablet", "latency_ms": 683, "is_exact": false, "is_alias": false, "fuzzy": 35, "is_correct": false, "input_tokens": 72, "output_tokens": 5, "total_tokens": 77}
{"id": "63", "question": "Unexpected travel invoice itemized by songs, maps, and barrels.", "answer_ref": "The Hobbit", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Magellan's Expedition (1519-1522)", "latency_ms": 890, "is_exact": false, "is_alias": false, "fuzzy": 10, "is_correct": false, "input_tokens": 77, "output_tokens": 18, "total_tokens": 95}
{"id": "60", "question": "Mountain resort with a generous winter rate for ghosts.", "answer_ref": "The Overlook Hotel", "aliases": ["Overlook Hotel"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Aspen (Colorado)", "latency_ms": 1116, "is_exact": false, "is_alias": false, "fuzzy": 42, "is_correct": false, "input_tokens": 74, "output_tokens": 7, "total_tokens": 81}
{"id": "64", "question": "Teen critiques phonies while auditioning for solitude.", "answer_ref": "The Catcher in the Rye", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The Matrix (movie)", "latency_ms": 1018, "is_exact": false, "is_alias": false, "fuzzy": 46, "is_correct": false, "input_tokens": 77, "output_tokens": 6, "total_tokens": 83}
{"id": "66", "question": "Opera, rock, and confession crammed into one late-night phone call.", "answer_ref": "Bohemian Rhapsody", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Samson_Awesome\n\nExplanation: This riddle refers to the character Samson from the biblical story, who was known for his strength (like rock), had long hair (which was his power, like opera arias), and confessed his sins to Delilah in a late-night phone call (symbolically, in this context). The term \"Awesome\" is a nickname often associated with Samson due to his superhuman strength.", "latency_ms": 1537, "is_exact": false, "is_alias": false, "fuzzy": 10, "is_correct": false, "input_tokens": 80, "output_tokens": 99, "total_tokens": 179}
{"id": "65", "question": "Musician studies physics to pronounce reality correctly.", "answer_ref": "The Name of the Wind", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Feynman", "latency_ms": 2271, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 73, "output_tokens": 4, "total_tokens": 77}
{"id": "67", "question": "Red jacket, moonwalk, and a zombie rehearsal dinner.", "answer_ref": "Thriller", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Thriller Music Video", "latency_ms": 1952, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 78, "output_tokens": 6, "total_tokens": 84}
{"id": "62", "question": "Dry world where rhythms of sand set politics and prophecy.", "answer_ref": "Dune", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Dune (novel)", "latency_ms": 3032, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 80, "output_tokens": 7, "total_tokens": 87}
{"id": "68", "question": "Promise-heavy synth pledge weaponized by surprise.", "answer_ref": "Never Gonna Give You Up", "aliases": ["Rickroll song"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Terminator 2: Judgment Day (movie)", "latency_ms": 1463, "is_exact": false, "is_alias": false, "fuzzy": 29, "is_correct": false, "input_tokens": 76, "output_tokens": 14, "total_tokens": 90}
{"id": "69", "question": "Water metaphors filed under overdue heartbreak invoices.", "answer_ref": "Rolling in the Deep", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Titanic", "latency_ms": 589, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false, "input_tokens": 76, "output_tokens": 3, "total_tokens": 79}
{"id": "73", "question": "Space crew plays HR's least favorite trust exercise.", "answer_ref": "Among Us", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Apollo 13", "latency_ms": 628, "is_exact": false, "is_alias": false, "fuzzy": 23, "is_correct": false, "input_tokens": 73, "output_tokens": 6, "total_tokens": 79}
{"id": "71", "question": "Falling real estate teaches ruthless minimalism.", "answer_ref": "Tetris", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Wright Brothers' Flyer", "latency_ms": 844, "is_exact": false, "is_alias": false, "fuzzy": 29, "is_correct": false, "input_tokens": 73, "output_tokens": 5, "total_tokens": 78}
{"id": "70", "question": "Horse-free riding lesson conducted in a tux.", "answer_ref": "Gangnam Style", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Moonlight Sonata", "latency_ms": 1107, "is_exact": false, "is_alias": false, "fuzzy": 34, "is_correct": false, "input_tokens": 75, "output_tokens": 5, "total_tokens": 80}
{"id": "75", "question": "Death-as-syllabus with bonfires for office hours.", "answer_ref": "Dark Souls", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Inferno (Dante's Divine Comedy)", "latency_ms": 393, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 79, "output_tokens": 14, "total_tokens": 93}
{"id": "72", "question": "Cubist wilderness where punching trees becomes architecture.", "answer_ref": "Minecraft", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Guernica", "latency_ms": 1105, "is_exact": false, "is_alias": false, "fuzzy": 35, "is_correct": false, "input_tokens": 75, "output_tokens": 4, "total_tokens": 79}
{"id": "74", "question": "Open-world archaeology of despair, punctuated by grace checkpoints.", "answer_ref": "Elden Ring", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Tomb Raider (game series)", "latency_ms": 1034, "is_exact": false, "is_alias": false, "fuzzy": 36, "is_correct": false, "input_tokens": 78, "output_tokens": 9, "total_tokens": 87}
{"id": "76", "question": "Adoption paperwork scattered across war and alchemy.", "answer_ref": "The Witcher 3: Wild Hunt", "aliases": ["The Witcher 3"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The Da Vinci Code", "latency_ms": 1132, "is_exact": false, "is_alias": false, "fuzzy": 31, "is_correct": false, "input_tokens": 75, "output_tokens": 6, "total_tokens": 81}
{"id": "78", "question": "Three unreliable narrators share a commute and a crime spreadsheet.", "answer_ref": "Grand Theft Auto V", "aliases": ["GTA V"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Memento", "latency_ms": 896, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false, "input_tokens": 79, "output_tokens": 4, "total_tokens": 83}
{"id": "79", "question": "Science fair judged by cake lore and lemons.", "answer_ref": "Portal 2", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Monty Python's Life of Brian", "latency_ms": 1067, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false, "input_tokens": 77, "output_tokens": 9, "total_tokens": 86}
{"id": "80", "question": "Interstellar HR balancing romance, diplomacy, and space bugs.", "answer_ref": "Mass Effect", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The Matrix", "latency_ms": 1419, "is_exact": false, "is_alias": false, "fuzzy": 23, "is_correct": false, "input_tokens": 77, "output_tokens": 2, "total_tokens": 79}
{"id": "77", "question": "Campfire ledger balances loyalty, debt, and westward entropy.", "answer_ref": "Red Dead Redemption 2", "aliases": ["RDR2"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The answer is: The Good, the Bad, and the Ugly. This is a spaghetti western film that features characters dealing with themes of loyalty, debt, and the westward expansion of the United States. The \"campfire\" and \"ledger\" are metaphors for the setting and the central conflict of the story.", "latency_ms": 2022, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false, "input_tokens": 78, "output_tokens": 71, "total_tokens": 149}
{"id": "81", "question": "Short on continents, tall on ambitions and tailored coats.", "answer_ref": "Napoleon Bonaparte", "aliases": ["Napoleon"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Empire State Building", "latency_ms": 1218, "is_exact": false, "is_alias": false, "fuzzy": 41, "is_correct": false, "input_tokens": 77, "output_tokens": 3, "total_tokens": 80}
{"id": "82", "question": "Postal service enthusiast with aggressive merger policies.", "answer_ref": "Genghis Khan", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "United States Postal Service (USPS)", "latency_ms": 1504, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false, "input_tokens": 74, "output_tokens": 10, "total_tokens": 84}
{"id": "83", "question": "Teenage strategist whose mail came with flames.", "answer_ref": "Joan of Arc", "aliases": ["Jeanne d'Arc"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Harry Potter (Hogwarts letter)", "latency_ms": 1392, "is_exact": false, "is_alias": false, "fuzzy": 35, "is_correct": false, "input_tokens": 73, "output_tokens": 10, "total_tokens": 83}
{"id": "84", "question": "Glowing research with a half-life of acclaim.", "answer_ref": "Marie Curie", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Fission reactor (Nuclear reactor used in research)", "latency_ms": 1408, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false, "input_tokens": 75, "output_tokens": 15, "total_tokens": 90}
{"id": "86", "question": "One-eyed project manager who pays ravens in secrets.", "answer_ref": "Odin", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Moriarty (Sherlock Holmes villain)", "latency_ms": 1171, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false, "input_tokens": 75, "output_tokens": 13, "total_tokens": 88}
{"id": "85", "question": "Hair that suggests stormy weather for equations.", "answer_ref": "Albert Einstein", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Gauss (Johann Carl Friedrich Gauss) - His last name, Gauss, is often associated with the \"hair\" of a matrix in linear algebra, and he made significant contributions to mathematics and physics, including number theory and differential equations. The term \"Gaussian\" is used to describe statistical distributions that have a bell-shaped curve, which can be likened to stormy weather due to its peaks and valleys.", "latency_ms": 1465, "is_exact": false, "is_alias": false, "fuzzy": 8, "is_correct": false, "input_tokens": 72, "output_tokens": 91, "total_tokens": 163}
{"id": "87", "question": "Strategy consultant whose owl handles night shift.", "answer_ref": "Athena", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "McKinsey (Owl is the symbol for McKinsey & Company, a global management consulting firm)", "latency_ms": 1141, "is_exact": false, "is_alias": false, "fuzzy": 10, "is_correct": false, "input_tokens": 71, "output_tokens": 22, "total_tokens": 93}
{"id": "88", "question": "Underworld landlord with a pomegranate clause.", "answer_ref": "Hades", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Charon's Ferry (ferryman in Greek Underworld) with a condition for passage (pomegranate seeds in Greek mythology) = Charon's Ferry Toll", "latency_ms": 1153, "is_exact": false, "is_alias": false, "fuzzy": 9, "is_correct": false, "input_tokens": 75, "output_tokens": 41, "total_tokens": 116}
{"id": "89", "question": "Village rooflines under a swirling insomnia.", "answer_ref": "Starry Night", "aliases": ["The Starry Night"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Bellagio (Italian village known for its picturesque rooftops and its location near Lake Como, which is often shrouded in mist at night)", "latency_ms": 1098, "is_exact": false, "is_alias": false, "fuzzy": 58, "is_correct": false, "input_tokens": 75, "output_tokens": 34, "total_tokens": 109}
{"id": "90", "question": "Clocks take a nap over a hot afternoon.", "answer_ref": "The Persistence of Memory", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "SANDMAN (Clock with pendulum)", "latency_ms": 1109, "is_exact": false, "is_alias": false, "fuzzy": 29, "is_correct": false, "input_tokens": 72, "output_tokens": 10, "total_tokens": 82}
{"id": "92", "question": "Shiba evaluates life with quantifiers and Comic Sans.", "answer_ref": "Doge", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Silicon Valley", "latency_ms": 1071, "is_exact": false, "is_alias": false, "fuzzy": 22, "is_correct": false, "input_tokens": 77, "output_tokens": 4, "total_tokens": 81}
{"id": "91", "question": "Allegiance wanders mid-stride; captions do the rest.", "answer_ref": "Distracted Boyfriend", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "The Mona Lisa (Leonardo da Vinci's painting)", "latency_ms": 1393, "is_exact": false, "is_alias": false, "fuzzy": 27, "is_correct": false, "input_tokens": 78, "output_tokens": 16, "total_tokens": 94}
{"id": "93", "question": "Airborne accountant who tallied rings in Chicago.", "answer_ref": "Michael Jordan", "aliases": ["MJ"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Michael Jordan (Chicago Bulls basketball player, known for his \"Air Jordan\" brand and six NBA championship rings)", "latency_ms": 1196, "is_exact": false, "is_alias": false, "fuzzy": 100, "is_correct": true, "input_tokens": 74, "output_tokens": 25, "total_tokens": 99}
{"id": "94", "question": "Serve so decisive it files its own appeal.", "answer_ref": "Serena Williams", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Supreme Court", "latency_ms": 1614, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false, "input_tokens": 73, "output_tokens": 2, "total_tokens": 75}
{"id": "97", "question": "Patriotic discus enthusiast unfrozen for modern meetings.", "answer_ref": "Captain America", "aliases": ["Steve Rogers"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Samuel Adams", "latency_ms": 1089, "is_exact": false, "is_alias": false, "fuzzy": 33, "is_correct": false, "input_tokens": 77, "output_tokens": 3, "total_tokens": 80}
{"id": "96", "question": "Purple ethicist proposes resource management by finger interface.", "answer_ref": "Thanos", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Purple Monkey's Radio (character from the movie \"The Fifth Element\")", "latency_ms": 1344, "is_exact": false, "is_alias": false, "fuzzy": 16, "is_correct": false, "input_tokens": 76, "output_tokens": 17, "total_tokens": 93}
{"id": "95", "question": "Finish line receives delivery before the starting gun's echo.", "answer_ref": "Usain Bolt", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Indianapolis 500 (race)", "latency_ms": 1737, "is_exact": false, "is_alias": false, "fuzzy": 25, "is_correct": false, "input_tokens": 75, "output_tokens": 10, "total_tokens": 85}
{"id": "99", "question": "Imprisonment catered by dumplings; vengeance plated cold.", "answer_ref": "Oldboy", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Alcatraz (prison) served Jell-O (cold food).", "latency_ms": 1354, "is_exact": false, "is_alias": false, "fuzzy": 17, "is_correct": false, "input_tokens": 81, "output_tokens": 17, "total_tokens": 98}
{"id": "100", "question": "Typewriter productivity meets corridor geometry and mirrored warnings.", "answer_ref": "The Shining", "aliases": [], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "IBM Selectric", "latency_ms": 1174, "is_exact": false, "is_alias": false, "fuzzy": 20, "is_correct": false, "input_tokens": 75, "output_tokens": 4, "total_tokens": 79}
{"id": "98", "question": "Stand-up philosophy major specializing in citywide punchlines.", "answer_ref": "Joker", "aliases": ["The Joker"], "model": "openrouter:mistralai/mistral-7b-instruct", "answer": "Louie CK (Louis Szekely), a stand-up comedian known for his philosophical humor, often incorporates city-related jokes in his acts. However, it's important to note that he is not a major in philosophy, but he has been open about his self-study of the subject. Also, he is not known for citywide punchlines specifically, but his comedy often references cities, particularly New York City, where he lives and works.", "latency_ms": 3177, "is_exact": false, "is_alias": false, "fuzzy": 2, "is_correct": false, "input_tokens": 76, "output_tokens": 99, "total_tokens": 175}
