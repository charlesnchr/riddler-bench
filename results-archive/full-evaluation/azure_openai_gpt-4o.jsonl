{"id": "1", "question": "Movie about a man fighting for his life to gain freedom in a large building in an old European city?", "answer_ref": "Gladiator", "aliases": ["Ridley Scott's Gladiator"], "model": "azure_openai:gpt-4o", "answer": "The Great Escape", "latency_ms": 5109, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false}
{"id": "2", "question": "Film where a floating metal giant kisses ice and the seating plan gets very competitive with small boats.", "answer_ref": "Titanic", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Titanic", "latency_ms": 933, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "3", "question": "Story where a bored office worker learns gravity and bullets take suggestions because reality is a software demo.", "answer_ref": "The Matrix", "aliases": ["Matrix"], "model": "azure_openai:gpt-4o", "answer": "The Matrix", "latency_ms": 932, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "4", "question": "Man who treats a foggy capital as a crossword, using a hat, a pipe, and his roommate's patience.", "answer_ref": "Sherlock Holmes", "aliases": ["Holmes"], "model": "azure_openai:gpt-4o", "answer": "Sherlock Holmes", "latency_ms": 502, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "5", "question": "Breathing-challenged space dad with a glow stick and strict HR policies.", "answer_ref": "Darth Vader", "aliases": ["Anakin Skywalker"], "model": "azure_openai:gpt-4o", "answer": "Darth Vader", "latency_ms": 498, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "6", "question": "Elegant glowstick for a more civilized tantrum.", "answer_ref": "Lightsaber", "aliases": ["Light saber", "Laser sword"], "model": "azure_openai:gpt-4o", "answer": "Lightsaber", "latency_ms": 467, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "7", "question": "Professor who believes museums are hungry and snakes are colleagues.", "answer_ref": "Indiana Jones", "aliases": ["Dr. Jones", "Henry Jones Jr."], "model": "azure_openai:gpt-4o", "answer": "Indiana Jones", "latency_ms": 959, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "8", "question": "Painting of a woman who knows something you don't and refuses to tell.", "answer_ref": "Mona Lisa", "aliases": ["La Gioconda"], "model": "azure_openai:gpt-4o", "answer": "Mona Lisa", "latency_ms": 480, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "9", "question": "Roman who found stairs harder than politics.", "answer_ref": "Julius Caesar", "aliases": ["Caesar"], "model": "azure_openai:gpt-4o", "answer": "Claudius", "latency_ms": 510, "is_exact": false, "is_alias": false, "fuzzy": 57, "is_correct": false}
{"id": "10", "question": "Volcano used as a return desk for unwanted jewelry.", "answer_ref": "Mount Doom", "aliases": ["Orodruin"], "model": "azure_openai:gpt-4o", "answer": "Mount Doom", "latency_ms": 991, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "11", "question": "Accessory that makes you invisible, anxious, and very popular with spiders.", "answer_ref": "The One Ring", "aliases": ["One Ring", "Ring of Power"], "model": "azure_openai:gpt-4o", "answer": "One Ring", "latency_ms": 920, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "12", "question": "Scottish castle that doubles as a boarding school and an ongoing safety inspection.", "answer_ref": "Hogwarts", "aliases": ["Hogwarts School of Witchcraft and Wizardry"], "model": "azure_openai:gpt-4o", "answer": "Hogwarts", "latency_ms": 1213, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "13", "question": "Archer who turns televised camping into regime change.", "answer_ref": "Katniss Everdeen", "aliases": ["Katniss"], "model": "azure_openai:gpt-4o", "answer": "Katniss Everdeen", "latency_ms": 529, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "14", "question": "Movie where an orange-loving family negotiates with horses.", "answer_ref": "The Godfather", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Godfather", "latency_ms": 621, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "15", "question": "Series about a chemistry teacher who starts a blue boutique for adults.", "answer_ref": "Breaking Bad", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Breaking Bad", "latency_ms": 484, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "16", "question": "Blue closet that's bigger than it apologizes for.", "answer_ref": "TARDIS", "aliases": ["The TARDIS"], "model": "azure_openai:gpt-4o", "answer": "TARDIS", "latency_ms": 496, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "17", "question": "Mythic blade that chooses its owner via limestone custody.", "answer_ref": "Excalibur", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Excalibur", "latency_ms": 1103, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "18", "question": "Address in London that sees more violin practice than rent checks.", "answer_ref": "221B Baker Street", "aliases": ["221B"], "model": "azure_openai:gpt-4o", "answer": "221B Baker Street", "latency_ms": 538, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "19", "question": "Space golf ball with a hostile laser pointer.", "answer_ref": "Death Star", "aliases": ["The Death Star"], "model": "azure_openai:gpt-4o", "answer": "Death Star", "latency_ms": 497, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "20", "question": "Rodent who invoices in volts.", "answer_ref": "Pikachu", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Pikachu", "latency_ms": 488, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "21", "question": "Plumber who rescues monarchy using cardio and mushrooms.", "answer_ref": "Mario", "aliases": ["Super Mario"], "model": "azure_openai:gpt-4o", "answer": "Mario", "latency_ms": 1092, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "22", "question": "Band that turned a crosswalk into a landmark.", "answer_ref": "The Beatles", "aliases": ["Beatles"], "model": "azure_openai:gpt-4o", "answer": "The Beatles", "latency_ms": 484, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "23", "question": "Pair who invented traffic jams in the sky.", "answer_ref": "Wright brothers", "aliases": ["The Wright Brothers", "Orville and Wilbur Wright"], "model": "azure_openai:gpt-4o", "answer": "Wright Brothers", "latency_ms": 504, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "24", "question": "Queen who turned snakes into accessories.", "answer_ref": "Cleopatra", "aliases": ["Cleopatra VII"], "model": "azure_openai:gpt-4o", "answer": "Medusa", "latency_ms": 926, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false}
{"id": "25", "question": "Greek manager of lightning and elaborate disguises.", "answer_ref": "Zeus", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Zeus", "latency_ms": 476, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "26", "question": "Woman whose gaze doubles as a landscaping tool.", "answer_ref": "Medusa", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Medusa", "latency_ms": 483, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "27", "question": "Film where a CEO runs a Turing test that tries to leave the building.", "answer_ref": "Ex Machina", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Ex Machina", "latency_ms": 484, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "28", "question": "Heist that happens inside pillows.", "answer_ref": "Inception", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Inception", "latency_ms": 476, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "29", "question": "Movie about retirement planning for artificial people.", "answer_ref": "Blade Runner", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Blade Runner", "latency_ms": 500, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "30", "question": "Savannah Hamlet with more singing.", "answer_ref": "The Lion King", "aliases": ["Lion King"], "model": "azure_openai:gpt-4o", "answer": "The Lion King", "latency_ms": 969, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "31", "question": "Small green grammar instructor from space.", "answer_ref": "Yoda", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Yoda", "latency_ms": 572, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "32", "question": "Grey-bearded fireworks enthusiast with a famous bridge speech.", "answer_ref": "Gandalf", "aliases": ["Gandalf the Grey", "Gandalf the White"], "model": "azure_openai:gpt-4o", "answer": "Gandalf", "latency_ms": 967, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "33", "question": "Ship that makes 'parsecs' sound like speed.", "answer_ref": "Millennium Falcon", "aliases": ["The Millennium Falcon"], "model": "azure_openai:gpt-4o", "answer": "Millennium Falcon", "latency_ms": 550, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "34", "question": "Tool that won't consider your application unless you're worthy.", "answer_ref": "Mjölnir", "aliases": ["Mjolnir", "Thor's hammer"], "model": "azure_openai:gpt-4o", "answer": "Excalibur", "latency_ms": 521, "is_exact": false, "is_alias": false, "fuzzy": 37, "is_correct": false}
{"id": "35", "question": "Guide that insists towels are endgame gear.", "answer_ref": "The Hitchhiker's Guide to the Galaxy", "aliases": ["Hitchhiker's Guide"], "model": "azure_openai:gpt-4o", "answer": "The Hitchhiker's Guide to the Galaxy", "latency_ms": 616, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "36", "question": "Doctor who keeps notes and bullets for a detective.", "answer_ref": "Dr. Watson", "aliases": ["John Watson"], "model": "azure_openai:gpt-4o", "answer": "John Watson", "latency_ms": 505, "is_exact": false, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "37", "question": "Rock that turns lead into disappointment for alchemists who never find it.", "answer_ref": "Philosopher's Stone", "aliases": ["Sorcerer's Stone"], "model": "azure_openai:gpt-4o", "answer": "Philosopher's Stone", "latency_ms": 529, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "38", "question": "Long stone apology to nomads.", "answer_ref": "Great Wall of China", "aliases": ["The Great Wall"], "model": "azure_openai:gpt-4o", "answer": "Hadrian's Wall", "latency_ms": 975, "is_exact": false, "is_alias": false, "fuzzy": 60, "is_correct": false}
{"id": "39", "question": "Man assembled like flat-pack furniture, without the instructions.", "answer_ref": "Frankenstein's monster", "aliases": ["Frankenstein's creature", "The monster"], "model": "azure_openai:gpt-4o", "answer": "Frankenstein's Monster", "latency_ms": 512, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "40", "question": "Prince who holds more meetings with a skull than with HR.", "answer_ref": "Hamlet", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Hamlet", "latency_ms": 1095, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "41", "question": "Corsican who measured Russia the hard way.", "answer_ref": "Napoleon Bonaparte", "aliases": ["Napoleon"], "model": "azure_openai:gpt-4o", "answer": "Napoleon Bonaparte", "latency_ms": 816, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "42", "question": "Blue rock that took a very cold dive.", "answer_ref": "Heart of the Ocean", "aliases": ["The Heart of the Ocean"], "model": "azure_openai:gpt-4o", "answer": "Titanic", "latency_ms": 482, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false}
{"id": "43", "question": "Car that treats alleys like runways and bats like a brand.", "answer_ref": "Batmobile", "aliases": ["The Batmobile"], "model": "azure_openai:gpt-4o", "answer": "Batmobile", "latency_ms": 489, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "44", "question": "Manager who prefers wearable tech for surveillance.", "answer_ref": "Sauron", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Big Brother", "latency_ms": 485, "is_exact": false, "is_alias": false, "fuzzy": 23, "is_correct": false}
{"id": "45", "question": "Moon where tall blue cat-people argue with bulldozers.", "answer_ref": "Avatar", "aliases": ["James Cameron's Avatar"], "model": "azure_openai:gpt-4o", "answer": "Pandora", "latency_ms": 944, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false}
{"id": "1", "question": "Movie about a man fighting for his life to gain freedom in a large building in an old European city?", "answer_ref": "Gladiator", "aliases": ["Ridley Scott's Gladiator"], "model": "azure_openai:gpt-4o", "answer": "The Great Escape", "latency_ms": 882, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false}
{"id": "2", "question": "Film where a floating metal giant kisses ice and the seating plan gets very competitive with small boats.", "answer_ref": "Titanic", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Titanic", "latency_ms": 757, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "3", "question": "Story where a bored office worker learns gravity and bullets take suggestions because reality is a software demo.", "answer_ref": "The Matrix", "aliases": ["Matrix"], "model": "azure_openai:gpt-4o", "answer": "The Matrix", "latency_ms": 479, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "4", "question": "Man who treats a foggy capital as a crossword, using a hat, a pipe, and his roommate's patience.", "answer_ref": "Sherlock Holmes", "aliases": ["Holmes"], "model": "azure_openai:gpt-4o", "answer": "Sherlock Holmes", "latency_ms": 496, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "5", "question": "Breathing-challenged space dad with a glow stick and strict HR policies.", "answer_ref": "Darth Vader", "aliases": ["Anakin Skywalker"], "model": "azure_openai:gpt-4o", "answer": "Darth Vader", "latency_ms": 942, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "6", "question": "Elegant glowstick for a more civilized tantrum.", "answer_ref": "Lightsaber", "aliases": ["Light saber", "Laser sword"], "model": "azure_openai:gpt-4o", "answer": "Lightsaber", "latency_ms": 478, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "7", "question": "Professor who believes museums are hungry and snakes are colleagues.", "answer_ref": "Indiana Jones", "aliases": ["Dr. Jones", "Henry Jones Jr."], "model": "azure_openai:gpt-4o", "answer": "Indiana Jones", "latency_ms": 931, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "8", "question": "Painting of a woman who knows something you don't and refuses to tell.", "answer_ref": "Mona Lisa", "aliases": ["La Gioconda"], "model": "azure_openai:gpt-4o", "answer": "Mona Lisa", "latency_ms": 518, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "9", "question": "Roman who found stairs harder than politics.", "answer_ref": "Julius Caesar", "aliases": ["Caesar"], "model": "azure_openai:gpt-4o", "answer": "Julius Caesar", "latency_ms": 554, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "10", "question": "Volcano used as a return desk for unwanted jewelry.", "answer_ref": "Mount Doom", "aliases": ["Orodruin"], "model": "azure_openai:gpt-4o", "answer": "Mount Doom", "latency_ms": 473, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "11", "question": "Accessory that makes you invisible, anxious, and very popular with spiders.", "answer_ref": "The One Ring", "aliases": ["One Ring", "Ring of Power"], "model": "azure_openai:gpt-4o", "answer": "One Ring", "latency_ms": 477, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "12", "question": "Scottish castle that doubles as a boarding school and an ongoing safety inspection.", "answer_ref": "Hogwarts", "aliases": ["Hogwarts School of Witchcraft and Wizardry"], "model": "azure_openai:gpt-4o", "answer": "Hogwarts", "latency_ms": 479, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "13", "question": "Archer who turns televised camping into regime change.", "answer_ref": "Katniss Everdeen", "aliases": ["Katniss"], "model": "azure_openai:gpt-4o", "answer": "Katniss Everdeen", "latency_ms": 497, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "14", "question": "Movie where an orange-loving family negotiates with horses.", "answer_ref": "The Godfather", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "The Godfather", "latency_ms": 468, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "15", "question": "Series about a chemistry teacher who starts a blue boutique for adults.", "answer_ref": "Breaking Bad", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Breaking Bad", "latency_ms": 2778, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "16", "question": "Blue closet that's bigger than it apologizes for.", "answer_ref": "TARDIS", "aliases": ["The TARDIS"], "model": "azure_openai:gpt-4o", "answer": "TARDIS", "latency_ms": 499, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "17", "question": "Mythic blade that chooses its owner via limestone custody.", "answer_ref": "Excalibur", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Excalibur", "latency_ms": 487, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "18", "question": "Address in London that sees more violin practice than rent checks.", "answer_ref": "221B Baker Street", "aliases": ["221B"], "model": "azure_openai:gpt-4o", "answer": "221B Baker Street", "latency_ms": 505, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "19", "question": "Space golf ball with a hostile laser pointer.", "answer_ref": "Death Star", "aliases": ["The Death Star"], "model": "azure_openai:gpt-4o", "answer": "Death Star", "latency_ms": 479, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "20", "question": "Rodent who invoices in volts.", "answer_ref": "Pikachu", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Pikachu", "latency_ms": 925, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "21", "question": "Plumber who rescues monarchy using cardio and mushrooms.", "answer_ref": "Mario", "aliases": ["Super Mario"], "model": "azure_openai:gpt-4o", "answer": "Mario", "latency_ms": 469, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "22", "question": "Band that turned a crosswalk into a landmark.", "answer_ref": "The Beatles", "aliases": ["Beatles"], "model": "azure_openai:gpt-4o", "answer": "The Beatles", "latency_ms": 488, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "23", "question": "Pair who invented traffic jams in the sky.", "answer_ref": "Wright brothers", "aliases": ["The Wright Brothers", "Orville and Wilbur Wright"], "model": "azure_openai:gpt-4o", "answer": "Wright Brothers", "latency_ms": 938, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "24", "question": "Queen who turned snakes into accessories.", "answer_ref": "Cleopatra", "aliases": ["Cleopatra VII"], "model": "azure_openai:gpt-4o", "answer": "Medusa", "latency_ms": 532, "is_exact": false, "is_alias": false, "fuzzy": 26, "is_correct": false}
{"id": "25", "question": "Greek manager of lightning and elaborate disguises.", "answer_ref": "Zeus", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Zeus", "latency_ms": 1125, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "26", "question": "Woman whose gaze doubles as a landscaping tool.", "answer_ref": "Medusa", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Medusa", "latency_ms": 488, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "27", "question": "Film where a CEO runs a Turing test that tries to leave the building.", "answer_ref": "Ex Machina", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Ex Machina", "latency_ms": 474, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "28", "question": "Heist that happens inside pillows.", "answer_ref": "Inception", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Inception", "latency_ms": 490, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "29", "question": "Movie about retirement planning for artificial people.", "answer_ref": "Blade Runner", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Blade Runner", "latency_ms": 482, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "30", "question": "Savannah Hamlet with more singing.", "answer_ref": "The Lion King", "aliases": ["Lion King"], "model": "azure_openai:gpt-4o", "answer": "The Lion King", "latency_ms": 507, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "31", "question": "Small green grammar instructor from space.", "answer_ref": "Yoda", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Yoda", "latency_ms": 604, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "32", "question": "Grey-bearded fireworks enthusiast with a famous bridge speech.", "answer_ref": "Gandalf", "aliases": ["Gandalf the Grey", "Gandalf the White"], "model": "azure_openai:gpt-4o", "answer": "Gandalf", "latency_ms": 508, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "33", "question": "Ship that makes 'parsecs' sound like speed.", "answer_ref": "Millennium Falcon", "aliases": ["The Millennium Falcon"], "model": "azure_openai:gpt-4o", "answer": "Millennium Falcon", "latency_ms": 502, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "34", "question": "Tool that won't consider your application unless you're worthy.", "answer_ref": "Mjölnir", "aliases": ["Mjolnir", "Thor's hammer"], "model": "azure_openai:gpt-4o", "answer": "Excalibur", "latency_ms": 495, "is_exact": false, "is_alias": false, "fuzzy": 37, "is_correct": false}
{"id": "35", "question": "Guide that insists towels are endgame gear.", "answer_ref": "The Hitchhiker's Guide to the Galaxy", "aliases": ["Hitchhiker's Guide"], "model": "azure_openai:gpt-4o", "answer": "The Hitchhiker's Guide to the Galaxy", "latency_ms": 585, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "36", "question": "Doctor who keeps notes and bullets for a detective.", "answer_ref": "Dr. Watson", "aliases": ["John Watson"], "model": "azure_openai:gpt-4o", "answer": "John Watson", "latency_ms": 479, "is_exact": false, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "37", "question": "Rock that turns lead into disappointment for alchemists who never find it.", "answer_ref": "Philosopher's Stone", "aliases": ["Sorcerer's Stone"], "model": "azure_openai:gpt-4o", "answer": "Philosopher's Stone", "latency_ms": 499, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "38", "question": "Long stone apology to nomads.", "answer_ref": "Great Wall of China", "aliases": ["The Great Wall"], "model": "azure_openai:gpt-4o", "answer": "Hadrian's Wall", "latency_ms": 481, "is_exact": false, "is_alias": false, "fuzzy": 60, "is_correct": false}
{"id": "39", "question": "Man assembled like flat-pack furniture, without the instructions.", "answer_ref": "Frankenstein's monster", "aliases": ["Frankenstein's creature", "The monster"], "model": "azure_openai:gpt-4o", "answer": "Frankenstein's Monster", "latency_ms": 513, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "40", "question": "Prince who holds more meetings with a skull than with HR.", "answer_ref": "Hamlet", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Hamlet", "latency_ms": 490, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "41", "question": "Corsican who measured Russia the hard way.", "answer_ref": "Napoleon Bonaparte", "aliases": ["Napoleon"], "model": "azure_openai:gpt-4o", "answer": "Napoleon Bonaparte", "latency_ms": 517, "is_exact": true, "is_alias": false, "fuzzy": 100, "is_correct": true}
{"id": "42", "question": "Blue rock that took a very cold dive.", "answer_ref": "Heart of the Ocean", "aliases": ["The Heart of the Ocean"], "model": "azure_openai:gpt-4o", "answer": "Titanic", "latency_ms": 952, "is_exact": false, "is_alias": false, "fuzzy": 28, "is_correct": false}
{"id": "43", "question": "Car that treats alleys like runways and bats like a brand.", "answer_ref": "Batmobile", "aliases": ["The Batmobile"], "model": "azure_openai:gpt-4o", "answer": "Batmobile", "latency_ms": 506, "is_exact": true, "is_alias": true, "fuzzy": 100, "is_correct": true}
{"id": "44", "question": "Manager who prefers wearable tech for surveillance.", "answer_ref": "Sauron", "aliases": [], "model": "azure_openai:gpt-4o", "answer": "Big Brother", "latency_ms": 501, "is_exact": false, "is_alias": false, "fuzzy": 23, "is_correct": false}
{"id": "45", "question": "Moon where tall blue cat-people argue with bulldozers.", "answer_ref": "Avatar", "aliases": ["James Cameron's Avatar"], "model": "azure_openai:gpt-4o", "answer": "Pandora", "latency_ms": 509, "is_exact": false, "is_alias": false, "fuzzy": 30, "is_correct": false}
